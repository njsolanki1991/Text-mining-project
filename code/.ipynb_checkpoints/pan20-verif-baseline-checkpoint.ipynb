{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pan20_verif_evaluator as evaluator\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(y, threshold=0.5):\n",
    "    y = np.array(y)\n",
    "    y = np.ma.fix_invalid(y, fill_value=threshold)\n",
    "    y[y >= threshold] = 1\n",
    "    y[y < threshold] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(true_y, pred_y):\n",
    "    \"\"\"\n",
    "    Calculates the AUC score (Area Under the Curve), a well-known\n",
    "    scalar evaluation score for binary classifiers. This score\n",
    "    also considers \"unanswered\" problem, where score = 0.5.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_scores : array [n_problems]\n",
    "        The predictions outputted by a verification system.\n",
    "        Assumes `0 >= prediction <=1`.\n",
    "    ground_truth_scores : array [n_problems]\n",
    "        The gold annotations provided for each problem.\n",
    "        Will typically be `0` or `1`.\n",
    "    Returns\n",
    "    ----------\n",
    "    auc = the Area Under the Curve.\n",
    "    References\n",
    "    ----------\n",
    "        E. Stamatatos, et al. Overview of the Author Identification\n",
    "        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return roc_auc_score(true_y, pred_y)\n",
    "    except ValueError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_at_1(true_y, pred_y, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the c@1 score, an evaluation method specific to the\n",
    "    PAN competition. This method rewards predictions which leave\n",
    "    some problems unanswered (score = 0.5). See:\n",
    "        A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n",
    "        In Proc. of the 49th Annual Meeting of the Association for\n",
    "        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_scores : array [n_problems]\n",
    "        The predictions outputted by a verification system.\n",
    "        Assumes `0 >= prediction <=1`.\n",
    "    ground_truth_scores : array [n_problems]\n",
    "        The gold annotations provided for each problem.\n",
    "        Will always be `0` or `1`.\n",
    "    Returns\n",
    "    ----------\n",
    "    c@1 = the c@1 measure (which accounts for unanswered\n",
    "        problems.)\n",
    "    References\n",
    "    ----------\n",
    "        - E. Stamatatos, et al. Overview of the Author Identification\n",
    "        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n",
    "        - A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n",
    "        In Proc. of the 49th Annual Meeting of the Association for\n",
    "        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n",
    "    \"\"\"\n",
    "\n",
    "    n = float(len(pred_y))\n",
    "    nc, nu = 0.0, 0.0\n",
    "\n",
    "    for gt_score, pred_score in zip(true_y, pred_y):\n",
    "        if pred_score == 0.5:\n",
    "            nu += 1\n",
    "        elif (pred_score > 0.5) == (gt_score > 0.5):\n",
    "            nc += 1.0\n",
    "    \n",
    "    return (1 / n) * (nc + (nu * nc / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(true_y, pred_y):\n",
    "    \"\"\"\n",
    "    Assesses verification performance, assuming that every\n",
    "    `score > 0.5` represents a same-author pair decision.\n",
    "    Note that all non-decisions (scores == 0.5) are ignored\n",
    "    by this metric.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_scores : array [n_problems]\n",
    "        The predictions outputted by a verification system.\n",
    "        Assumes `0 >= prediction <=1`.\n",
    "    ground_truth_scores : array [n_problems]\n",
    "        The gold annotations provided for each problem.\n",
    "        Will typically be `0` or `1`.\n",
    "    Returns\n",
    "    ----------\n",
    "    acc = The number of correct attributions.\n",
    "    References\n",
    "    ----------\n",
    "        E. Stamatatos, et al. Overview of the Author Identification\n",
    "        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n",
    "    \"\"\"\n",
    "    true_y_filtered, pred_y_filtered = [], []\n",
    "\n",
    "    for true, pred in zip(true_y, pred_y):\n",
    "        if pred != 0.5:\n",
    "            true_y_filtered.append(true)\n",
    "            pred_y_filtered.append(pred)\n",
    "    \n",
    "    pred_y_filtered = binarize(pred_y_filtered)\n",
    "\n",
    "    return f1_score(true_y_filtered, pred_y_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_05_u_score(true_y, pred_y, pos_label=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Return F0.5u score of prediction.\n",
    "    :param true_y: true labels\n",
    "    :param pred_y: predicted labels\n",
    "    :param threshold: indication for non-decisions (default = 0.5)\n",
    "    :param pos_label: positive class label (default = 1)\n",
    "    :return: F0.5u score\n",
    "    \"\"\"\n",
    "\n",
    "    pred_y = binarize(pred_y)\n",
    "\n",
    "    n_tp = 0\n",
    "    n_fn = 0\n",
    "    n_fp = 0\n",
    "    n_u = 0\n",
    "\n",
    "    for i, pred in enumerate(pred_y):\n",
    "        if pred == threshold:\n",
    "            n_u += 1\n",
    "        elif pred == pos_label and pred == true_y[i]:\n",
    "            n_tp += 1\n",
    "        elif pred == pos_label and pred != true_y[i]:\n",
    "            n_fp += 1\n",
    "        elif true_y[i] == pos_label and pred != true_y[i]:\n",
    "            n_fn += 1\n",
    "\n",
    "    return (1.25 * n_tp) / (1.25 * n_tp + 0.25 * (n_fn + n_u) + n_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(fn):\n",
    "    problems = {}\n",
    "    for line in open(fn):\n",
    "        d =  json.loads(line.strip())\n",
    "        if 'value' in d:\n",
    "            problems[d['id']] = d['value']\n",
    "        else:\n",
    "            problems[d['id']] = int(d['same'])\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(true_y, pred_y):\n",
    "    \"\"\"\n",
    "    Convenience function: calculates all PAN20 evaluation measures\n",
    "    and returns them as a dict, including the 'overall' score, which\n",
    "    is the mean of the individual metrics (0 >= metric >= 1). All \n",
    "    scores get rounded to three digits.\n",
    "    \"\"\"\n",
    "\n",
    "    results = {'auc': auc(true_y, pred_y),\n",
    "               'c@1': c_at_1(true_y, pred_y),\n",
    "               'f_05_u': f_05_u_score(true_y, pred_y),\n",
    "               'F1': f1(true_y, pred_y)}\n",
    "    \n",
    "    results['overall'] = np.mean(list(results.values()))\n",
    "\n",
    "    for k, v in results.items():\n",
    "        results[k] = round(v, 3)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python\n",
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# \"\"\"\n",
    "# # Naive, Distance-Based Baseline\n",
    "# ## Introduction\n",
    "# This baseline offers a naive, yet fast solution to the \n",
    "# PAN2020 track on authorship verification. All documents\n",
    "# are represented using a bag-of-character-ngrams model,\n",
    "# that is TFIDF weighted. The cosine similarity between\n",
    "# each document pair in the calibration data set is\n",
    "# calculated. Finally, the resulting similarities are\n",
    "# optimized, and projected through a simple rescaling\n",
    "# operation, so that they can function as pseudo-probabi-\n",
    "# lities, indiciating the likelihood that a document-pair\n",
    "# is a same-author pair. Via a grid search, the optimal\n",
    "# verification threshold is determined, taking into account\n",
    "# that some difficult problems can be left unanswered.\n",
    "# Through setting `num_iterations` to an integer > 0,\n",
    "# a bootstrapped variant of this procedure can be used.\n",
    "# In this case, the similarity calculation is applied in\n",
    "# an iterative procedure to a randomly sampled subset of\n",
    "# the available features. The average similarity is then\n",
    "# used downstream. This imputation procedure is inspired\n",
    "# by the imposters approach.\n",
    "# ## Dependencies\n",
    "# - Python 3.6+ (we recommend the Anaconda Python distribution)\n",
    "# - scikit-learn, numpy, scipy\n",
    "# - non-essential: tqdm, seaborn/matplotlib\n",
    "# - pan20_verif_evaluator.py\n",
    "# Example usage from the command line:\n",
    "# >>> python pan20-verif-baseline.py \\\n",
    "#           -input_pairs=\"datasets/pan20-authorship-verification-training-small/pairs.jsonl\" \\\n",
    "#           -input_truth=\"datasets/pan20-authorship-verification-training-small/truth.jsonl\" \\\n",
    "#           -test_pairs=\"datasets/pan20-authorship-verification-test/pairs.jsonl\" \\\n",
    "#           -num_iterations=0 \\\n",
    "#           -output=\"out\"\n",
    "# \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-16b53f84fa84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkdeplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# from pan20_verif_evaluator import evaluate_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import kdeplot\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from pan20_verif_evaluator import evaluate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(value, orig_min, orig_max, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Rescales a `value` in the old range defined by\n",
    "    `orig_min` and `orig_max`, to the new range\n",
    "    `new_min` and `new_max`. Assumes that\n",
    "    `orig_min` <= value <= `orig_max`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    value: float, default=None\n",
    "        The value to be rescaled.\n",
    "    orig_min: float, default=None\n",
    "        The minimum of the original range.\n",
    "    orig_max: float, default=None\n",
    "        The minimum of the original range.\n",
    "    new_min: float, default=None\n",
    "        The minimum of the new range.\n",
    "    new_max: float, default=None\n",
    "        The minimum of the new range.\n",
    "    Returns\n",
    "    ----------\n",
    "    new_value: float\n",
    "        The rescaled value.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_span = orig_max - orig_min\n",
    "    new_span = new_max - new_min\n",
    "\n",
    "    try:\n",
    "        scaled_value = float(value - orig_min) / float(orig_span)\n",
    "    except ZeroDivisionError:\n",
    "        orig_span += 1e-6\n",
    "        scaled_value = float(value - orig_min) / float(orig_span)\n",
    "\n",
    "    return new_min + (scaled_value * new_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_scores(scores, p1, p2):\n",
    "    new_scores = []\n",
    "    for sc in scores:\n",
    "        if sc <= p1:\n",
    "            sc = rescale(sc, 0, p1, 0, 0.49)\n",
    "            new_scores.append(sc)\n",
    "        elif sc > p1 and sc < p2:\n",
    "            new_scores.append(0.5)\n",
    "        else:\n",
    "            sc = rescale(sc, p2, 1, 0.51, 1)\n",
    "            new_scores.append(sc)\n",
    "    return np.array(new_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    input_pairs = '.\\\\Datasets\\\\pan20-authorship-verification-training\\\\pairs.jsonl'\n",
    "    input_truth = '.\\\\Datasets\\\\pan20-authorship-verification-training\\\\truth.jsonl'\n",
    "    test_pairs = '.\\\\Datasets\\\\pan20-authorship-verification-test\\\\pairs.jsonl'\n",
    "    output = '.\\\\Datasets\\\\pan20-baseline\\\\out'\n",
    "    seed = 2020\n",
    "    vocab_size= 3000\n",
    "    ngram_size=4\n",
    "    num_iterations=0\n",
    "    dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     parser = argparse.ArgumentParser(description='Distance-based verification: PAN20 baseline')\n",
    "\n",
    "#     # data settings:\n",
    "#     parser.add_argument('-input_pairs', type=str, required=True,\n",
    "#                         help='Path to the jsonl-file with the input pairs')\n",
    "#     parser.add_argument('-input_truth', type=str, required=True,\n",
    "#                         help='Path to the ground truth-file for the input pairs')\n",
    "#     parser.add_argument('-test_pairs', type=str, required=True,\n",
    "#                         help='Path to the jsonl-file with the test pairs')\n",
    "#     parser.add_argument('-output', type=str, required=True,\n",
    "#                         help='Path to the output folder for the predictions.\\\n",
    "#                              (Will be overwritten if it exist already.)')\n",
    "\n",
    "#     # algorithmic settings:\n",
    "#     parser.add_argument('-seed', default=2020, type=int,\n",
    "#                         help='Random seed')\n",
    "#     parser.add_argument('-vocab_size', default=3000, type=int,\n",
    "#                         help='Maximum number of vocabulary items in feature space')\n",
    "#     parser.add_argument('-ngram_size', default=4, type=int,\n",
    "#                         help='Size of the ngrams')\n",
    "#     parser.add_argument('-num_iterations', default=0, type=int,\n",
    "#                         help='Number of iterations (`k`); zero by default')\n",
    "#     parser.add_argument('-dropout', default=.5, type=float,\n",
    "#                         help='Proportion of features to keep in each iteration')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    args = Args()\n",
    "    print(args)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(args.output)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    os.mkdir(args.output)\n",
    "\n",
    "    gold = {}\n",
    "    for line in open(args.input_truth):\n",
    "        d = json.loads(line.strip())\n",
    "        gold[d['id']] = int(d['same'])\n",
    "    print(len(gold))\n",
    "    # truncation for development purposes\n",
    "    cutoff = 0\n",
    "    if cutoff:\n",
    "        gold = dict(random.sample(gold.items(), cutoff))\n",
    "        print(len(gold))\n",
    "\n",
    "    texts = []\n",
    "    for line in tqdm(open(args.input_pairs)):\n",
    "        d = json.loads(line.strip())        \n",
    "        if d['id'] in gold:            \n",
    "            texts.extend(d['pair'])\n",
    "\n",
    "    print('-> constructing vectorizer')\n",
    "    vectorizer = TfidfVectorizer(max_features=args.vocab_size, analyzer='char',\n",
    "                                 ngram_range=(args.ngram_size, args.ngram_size))\n",
    "    print(len(texts))\n",
    "    vectorizer.fit(texts)\n",
    "\n",
    "    if args.num_iterations:\n",
    "        total_feats = len(vectorizer.get_feature_names())\n",
    "        keep_feats = int(total_feats * args.dropout)\n",
    "\n",
    "        rnd_feature_idxs = []\n",
    "        for _ in range(args.num_iterations):\n",
    "            rnd_feature_idxs.append(np.random.choice(total_feats,\n",
    "                                                     keep_feats,\n",
    "                                                     replace=False))\n",
    "        rnd_feature_idxs = np.array(rnd_feature_idxs)\n",
    "\n",
    "    print('-> calculating pairwise similarities')\n",
    "    similarities, labels = [], []\n",
    "    for line in tqdm(open(args.input_pairs)):\n",
    "        d = json.loads(line.strip())\n",
    "        if d['id'] in gold:\n",
    "            x1, x2 = vectorizer.transform(d['pair']).toarray()\n",
    "            if args.num_iterations:\n",
    "                similarities_ = []\n",
    "                for i in range(args.num_iterations):\n",
    "                    similarities_.append(cosine_sim(x1[rnd_feature_idxs[i, :]],\n",
    "                                                    x2[rnd_feature_idxs[i, :]]))\n",
    "                similarities.append(np.mean(similarities_))\n",
    "            else:\n",
    "                similarities.append(cosine_sim(x1, x2))\n",
    "            labels.append(gold[d['id']])\n",
    "    \n",
    "    similarities = np.array(similarities, dtype=np.float64)\n",
    "    labels = np.array(labels, dtype=np.float64)\n",
    "\n",
    "    kdeplot(similarities, label='orig cos sim')\n",
    "\n",
    "    print('-> grid search p1/p2:')\n",
    "    step_size = 0.01\n",
    "    thresholds = np.arange(0.01, 0.99, step_size)\n",
    "    combs = [(p1, p2) for (p1, p2) in combinations(thresholds, 2) if p1 < p2]\n",
    "\n",
    "    params = {}\n",
    "    for p1, p2 in tqdm(combs):\n",
    "        corrected_scores = correct_scores(similarities, p1=p1, p2=p2)\n",
    "        score = evaluate_all(pred_y=corrected_scores,\n",
    "                             true_y=labels)\n",
    "        params[(p1, p2)] = score['overall']\n",
    "    opt_p1, opt_p2 = max(params, key=params.get)\n",
    "    print('optimal p1/p2:', opt_p1, opt_p2)\n",
    "    plt.axvline(opt_p1, ls='--', c='darkgrey')\n",
    "    plt.axvline(opt_p2, ls='--', c='darkgrey')\n",
    "    \n",
    "    corrected_scores = correct_scores(similarities, p1=opt_p1, p2=opt_p2)\n",
    "    print('optimal score:', evaluate_all(pred_y=corrected_scores,\n",
    "                                         true_y=labels))\n",
    "    kdeplot(corrected_scores, label='corr cos sim')\n",
    "    corr_p1, corr_p2 = correct_scores([opt_p1, opt_p2], p1=opt_p1, p2=opt_p2)\n",
    "    plt.axvline(corr_p1, ls='--', c='darkgrey')\n",
    "    plt.axvline(corr_p2, ls='--', c='darkgrey')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kde.pdf')\n",
    "    plt.clf()\n",
    "    \n",
    "    print('-> determining optimal threshold')\n",
    "    scores = []\n",
    "    for th in np.linspace(0.25, 0.75, 1000):\n",
    "        adjusted = (corrected_scores >= th) * 1\n",
    "        scores.append((th,\n",
    "                        f1_score(labels, adjusted),\n",
    "                        precision_score(labels, adjusted),\n",
    "                        recall_score(labels, adjusted)))\n",
    "    thresholds, f1s, precisions, recalls = zip(*scores)\n",
    "\n",
    "    max_idx = np.array(f1s).argmax()\n",
    "    max_f1 = f1s[max_idx]\n",
    "    max_th = thresholds[max_idx]\n",
    "    print(f'Dev results -> F1={max_f1} at th={max_th}')\n",
    "\n",
    "    plt.plot(thresholds, precisions, label='precision')\n",
    "    plt.plot(thresholds, recalls, label='recall')\n",
    "    plt.plot(thresholds, f1s, label='F1')\n",
    "    plt.axvline(max_th, ls='-', c='darkgrey')\n",
    "    plt.xlim([0,1])\n",
    "    plt.gca().set_xlabel('theta')\n",
    "    plt.gca().legend()\n",
    "    plt.gca().set_facecolor('lightgrey')\n",
    "    plt.title(f'f1={round(max_f1, 4)} @ theta={round(max_th, 4)}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dev_precrec.pdf')\n",
    "\n",
    "    print('-> calculating test similarities')\n",
    "    with open(args.output + os.sep + 'answers.jsonl', 'w') as outf:\n",
    "        for line in tqdm(open(args.test_pairs)):\n",
    "            d = json.loads(line.strip())\n",
    "            problem_id = d['id']\n",
    "            x1, x2 = vectorizer.transform(d['pair']).toarray()\n",
    "            if args.num_iterations:\n",
    "                similarities_ = []\n",
    "                for i in range(args.num_iterations):\n",
    "                    similarities_.append(cosine_sim(x1[rnd_feature_idxs[i, :]],\n",
    "                                             x2[rnd_feature_idxs[i, :]]))\n",
    "                    similarity = np.mean(similarities_)\n",
    "            else:\n",
    "                similarity = cosine_sim(x1, x2)\n",
    "            \n",
    "            similarity = correct_scores([similarity], p1=opt_p1, p2=opt_p2)[0]\n",
    "            r = {'id': problem_id, 'value': similarity}\n",
    "            outf.write(json.dumps(r) + '\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorArgs:\n",
    "    i = '.\\\\Datasets\\\\pan20-authorship-verification-test'\n",
    "    a = '.\\\\Datasets\\\\pan20-baseline\\\\out'\n",
    "    o = '.\\\\Datasets\\\\pan20-baseline\\\\out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.main(EvaluatorArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
