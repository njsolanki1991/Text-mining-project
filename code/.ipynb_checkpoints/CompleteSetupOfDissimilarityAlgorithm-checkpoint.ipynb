{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import json\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "# For plotting of data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# For dataframes\n",
    "import pandas as pd\n",
    "# For Min-Max Normalisation\n",
    "from sklearn import preprocessing\n",
    "# For accessing file and folder paths\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "# For importing bigrams and trigrams from nltk\n",
    "from nltk.util import bigrams,trigrams\n",
    "\n",
    "# For Evaluation from PAN20 challenge\n",
    "import pan20_verif_evaluator as evaluator\n",
    "from time import perf_counter\n",
    "# Used for anlaysis of program\n",
    "import cProfile\n",
    "# Getting Date and Time information\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening Files and loading data\n",
    "def LoadAllData(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        JsonList = list(json_file)\n",
    "    return JsonList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function counts dissimilarity between two lists of freq dist - Performance slower than the below one\n",
    "# def DissimilarityAlgorithm(FirstList,SecondList):\n",
    "#     sumOfDissimilarValues = 0\n",
    "#     countOfSimilarValues = 0\n",
    "#     for firstItem in FirstList:\n",
    "#         for secondItem in SecondList:            \n",
    "#             if firstItem[0] == secondItem[0]:\n",
    "#                 # For items are similar, dissimilarities based on the frequencies\n",
    "#                 dissimilarValue = ((firstItem[1] - secondItem[1])*2/(firstItem[1] + secondItem[1]))**2\n",
    "#                 sumOfDissimilarValues = sumOfDissimilarValues + dissimilarValue\n",
    "#                 countOfSimilarValues = countOfSimilarValues + 1\n",
    "#     # For all items\n",
    "#     totalDissimilarValue = sumOfDissimilarValues + (len(FirstList)+len(SecondList)-countOfSimilarValues*2)*4\n",
    "#     return totalDissimilarValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function counts dissimilarity between two lists of freq dist- Better performance\n",
    "def DissimilarityAlgorithm(FirstList,SecondList):\n",
    "    sumOfDissimilarValues = 0\n",
    "    countOfSimilarValues = 0\n",
    "    temporaryList = []\n",
    "    for item in SecondList:\n",
    "        temporaryList.append(item[0])\n",
    "        \n",
    "    for firstItem in FirstList:\n",
    "        if(firstItem[0] in temporaryList):\n",
    "            indexOfSecondList=temporaryList.index(firstItem[0])\n",
    "            secondItem=SecondList[indexOfSecondList]\n",
    "            dissimilarValue = ((firstItem[1] - secondItem[1])*2/(firstItem[1] + secondItem[1]))**2\n",
    "            sumOfDissimilarValues = sumOfDissimilarValues + dissimilarValue\n",
    "            countOfSimilarValues = countOfSimilarValues + 1  \n",
    "            \n",
    "    # For all items\n",
    "    totalDissimilarValue = sumOfDissimilarValues + (len(FirstList)+len(SecondList)-countOfSimilarValues*2)*4\n",
    "    return totalDissimilarValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates POS-Tag ConditionalFrequency for the document with special modifications\n",
    "def EvaluatePOSTagConditionalFrequencyList(DocumentTagset, tag, mostFrequent='DEFAULT'):        \n",
    "    documentCfd=nltk.ConditionalFreqDist((tag,word) for (word,tag) in DocumentTagset)        \n",
    "    if(mostFrequent =='DEFAULT'):\n",
    "        profilelength=len(documentCfd[tag])\n",
    "    else:\n",
    "        profilelength=mostFrequent\n",
    "    return documentCfd[tag].most_common(profilelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits and get either part or gets the whole document depending upon conditions\n",
    "def FilterDocument(document,divisionType='FULL'):\n",
    "    filteredData=''\n",
    "    if(divisionType=='FIRSTHALF'):\n",
    "        filteredData = document[:int(len(document)/2)]\n",
    "    elif (divisionType=='SECONDHALF'):    \n",
    "        filteredData = document[int(len(document)/2):] \n",
    "    else:     \n",
    "        filteredData = document\n",
    "    return filteredData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates dissimilarity score for POS tags in the known and unknown document\n",
    "def EvaluatePosTagCount(DocumentTagset):\n",
    "    documentFreqDistOfPosTag = nltk.FreqDist(DocumentTagset)\n",
    "    documentCfdOfPosTag = nltk.FreqDist(tag for (word, tag) in documentFreqDistOfPosTag)\n",
    "    listOfMostCommonPosTagForDocument = documentCfdOfPosTag.most_common()\n",
    "    return listOfMostCommonPosTagForDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates character n-gram ConditionalFrequency for the document\n",
    "def EvaluateCharacterNGramConditionalFrequencyList(document, CharacterLength):\n",
    "    characterNGram = [word.lower() for word in [document[item:item+CharacterLength] for item in range(len(document)-CharacterLength+1)]]\n",
    "    return nltk.FreqDist(characterNGram).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the dissimilarities values for all features \n",
    "# namely PosTag for Verb,Noun,Pronoun and Adjective ,\n",
    "# Word-N-Gram for N varies from 1 to 3 for profile length 100 and 200, \n",
    "# Character-N-Gram for N varies from 4 to 7 for profile length 100 and 200 and \n",
    "# POSCount for all possble POS\n",
    "# and stores in a dataframe and then combining to the ground truth values \n",
    "# whether the documents are written by same or different author \n",
    "\n",
    "def EvaluateDissimilaritiesFullDocumentSecondProcedure(TrainingDataList,GroundTruthDataList):\n",
    "    dissimilarityValues = []\n",
    "    for item in TrainingDataList:\n",
    "                \n",
    "        unknownDocument = FilterDocument(json.loads(item)['pair'][1],'FULL')\n",
    "        knownDocument = FilterDocument(json.loads(item)['pair'][0],'FULL') \n",
    "        \n",
    "        unknownDocument = unknownDocument.replace('n\"t',' not').replace('N\"T',' NOT').replace('\"re',' are').replace('\"m',' am')\n",
    "        knownDocument = knownDocument.replace('n\"t',' not').replace('N\"T',' NOT').replace('\"re',' are').replace('\"m',' am')\n",
    "        \n",
    "        # Finding Tagset of documents\n",
    "        unknownDocumentTagset=nltk.pos_tag(word_tokenize(unknownDocument),tagset='universal')    \n",
    "        knownDocumentTagset=nltk.pos_tag(word_tokenize(knownDocument),tagset='universal')\n",
    "        \n",
    "        # Finding word-n-grams\n",
    "        unknownDocumentForWord1Gram=nltk.FreqDist(nltk.word_tokenize(unknownDocument)).most_common()\n",
    "        knownDocumentForWord1Gram=nltk.FreqDist(nltk.word_tokenize(knownDocument)).most_common()\n",
    "        unknownDocumentForWord2Gram=nltk.FreqDist(list(bigrams(nltk.word_tokenize(unknownDocument)))).most_common()\n",
    "        knownDocumentForWord2Gram=nltk.FreqDist(list(bigrams(nltk.word_tokenize(knownDocument)))).most_common()\n",
    "        unknownDocumentForWord3Gram=nltk.FreqDist(list(trigrams(nltk.word_tokenize(unknownDocument)))).most_common()\n",
    "        knownDocumentForWord3Gram=nltk.FreqDist(list(trigrams(nltk.word_tokenize(knownDocument)))).most_common() \n",
    "        \n",
    "        # Finding character-n-grams\n",
    "        unknownDocumentForCharacter4Gram=EvaluateCharacterNGramConditionalFrequencyList(unknownDocument, 4)\n",
    "        knownDocumentForCharacter4Gram=EvaluateCharacterNGramConditionalFrequencyList(knownDocument, 4)\n",
    "        unknownDocumentForCharacter5Gram=EvaluateCharacterNGramConditionalFrequencyList(unknownDocument, 5)\n",
    "        knownDocumentForCharacter5Gram=EvaluateCharacterNGramConditionalFrequencyList(knownDocument, 5)\n",
    "        \n",
    "        unknownDocumentForCharacter6Gram=EvaluateCharacterNGramConditionalFrequencyList(unknownDocument, 6)\n",
    "        knownDocumentForCharacter6Gram=EvaluateCharacterNGramConditionalFrequencyList(knownDocument, 6)\n",
    "        unknownDocumentForCharacter7Gram=EvaluateCharacterNGramConditionalFrequencyList(unknownDocument, 7)\n",
    "        knownDocumentForCharacter7Gram=EvaluateCharacterNGramConditionalFrequencyList(knownDocument, 7)\n",
    "        unknownDocumentForCharacter8Gram=EvaluateCharacterNGramConditionalFrequencyList(unknownDocument, 8)\n",
    "        knownDocumentForCharacter8Gram=EvaluateCharacterNGramConditionalFrequencyList(knownDocument, 8)\n",
    "        \n",
    "        \n",
    "        # For POS-Tag Feature\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Verb Feature Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForPosTagVerb=EvaluatePOSTagConditionalFrequencyList(unknownDocumentTagset,'VERB',100)        \n",
    "        knownDocumentCfdForPosTagVerb=EvaluatePOSTagConditionalFrequencyList(knownDocumentTagset,'VERB',100)\n",
    "        dissimilarityForPosTagVerb=DissimilarityAlgorithm(unknownDocumentCfdForPosTagVerb,knownDocumentCfdForPosTagVerb)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Verb Feature Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Noun Feature Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForPosTagNoun=EvaluatePOSTagConditionalFrequencyList(unknownDocumentTagset,'NOUN',100)        \n",
    "        knownDocumentCfdForPosTagNoun=EvaluatePOSTagConditionalFrequencyList(knownDocumentTagset,'NOUN',100)\n",
    "        dissimilarityForPosTagNoun=DissimilarityAlgorithm(unknownDocumentCfdForPosTagNoun,knownDocumentCfdForPosTagNoun)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Verb Feature Ends\n",
    "        # ----------------------------------------------------------------------------------------------------------- \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Pronoun Feature Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------        \n",
    "        unknownDocumentCfdForPosTagPronoun=EvaluatePOSTagConditionalFrequencyList(unknownDocumentTagset,'PRON',100)        \n",
    "        knownDocumentCfdForPosTagPronoun=EvaluatePOSTagConditionalFrequencyList(knownDocumentTagset,'PRON',100)\n",
    "        dissimilarityForPosTagPronoun=DissimilarityAlgorithm(unknownDocumentCfdForPosTagPronoun,knownDocumentCfdForPosTagPronoun)\n",
    "        # -----------------------------------------------------------------------------------------------------------        \n",
    "        # For Pronoun Feature Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------             \n",
    "        # For Adjective Feature Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForPosTagAdjective=EvaluatePOSTagConditionalFrequencyList(unknownDocumentTagset,'ADJ',100)        \n",
    "        knownDocumentCfdForPosTagAdjective=EvaluatePOSTagConditionalFrequencyList(knownDocumentTagset,'ADJ',100)\n",
    "        dissimilarityForPosTagAdjective=DissimilarityAlgorithm(unknownDocumentCfdForPosTagAdjective,knownDocumentCfdForPosTagAdjective)\n",
    "        # -----------------------------------------------------------------------------------------------------------             \n",
    "        # For Adjective Feature Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For word 1 gram for profile length 100 Starts\n",
    "        # ----------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "        \n",
    "        unknownDocumentCfdForWord1GramProfileLength100 = unknownDocumentForWord1Gram[:100]\n",
    "        knownDocumentCfdForWord1GramProfileLength100 = knownDocumentForWord1Gram[:100]\n",
    "        dissimilarityForWord1GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForWord1GramProfileLength100,knownDocumentCfdForWord1GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For word 1 gram for profile length 100 Ends\n",
    "        # ----------------------------------------------------------------------------------------------------------- \n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For word 2 gram for profile length 100 Starts\n",
    "        # ----------------------------------------------------------------------------------------------------------- \n",
    "        unknownDocumentCfdForWord2GramProfileLength100 = unknownDocumentForWord2Gram[:100]\n",
    "        knownDocumentCfdForWord2GramProfileLength100 = knownDocumentForWord2Gram[:100]\n",
    "        dissimilarityForWord2GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForWord2GramProfileLength100,knownDocumentCfdForWord2GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For word 2 gram for profile length 100 Ends\n",
    "        # ----------------------------------------------------------------------------------------------------------- \n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For word 3 gram for profile length 100 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForWord3GramProfileLength100 = unknownDocumentForWord3Gram[:100]\n",
    "        knownDocumentCfdForWord3GramProfileLength100 = knownDocumentForWord3Gram[:100]\n",
    "        dissimilarityForWord3GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForWord3GramProfileLength100,knownDocumentCfdForWord3GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For word 3 gram for profile length 100 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 4 gram for profile length 100 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter4GramProfileLength100 = unknownDocumentForCharacter4Gram[:100]\n",
    "        knownDocumentCfdForCharacter4GramProfileLength100 = knownDocumentForCharacter4Gram[:100]\n",
    "        dissimilarityCharacter4GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter4GramProfileLength100,knownDocumentCfdForCharacter4GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 4 gram for profile length 100 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 5 gram for profile length 100 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter5GramProfileLength100 = unknownDocumentForCharacter5Gram[:100]\n",
    "        knownDocumentCfdForCharacter5GramProfileLength100 =knownDocumentForCharacter5Gram[:100]\n",
    "        dissimilarityCharacter5GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter5GramProfileLength100,knownDocumentCfdForCharacter5GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 5 gram for profile length 100 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 6 gram for profile length 100 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter6GramProfileLength100 = unknownDocumentForCharacter6Gram[:100]\n",
    "        knownDocumentCfdForCharacter6GramProfileLength100 = knownDocumentForCharacter6Gram[:100]\n",
    "        dissimilarityCharacter6GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter6GramProfileLength100,knownDocumentCfdForCharacter6GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 6 gram for profile length 100 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 7 gram for profile length 100 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter7GramProfileLength100 = unknownDocumentForCharacter7Gram[:100]\n",
    "        knownDocumentCfdForCharacter7GramProfileLength100 = knownDocumentForCharacter7Gram[:100]\n",
    "        dissimilarityCharacter7GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter7GramProfileLength100,knownDocumentCfdForCharacter7GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 7 gram for profile length 100 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 8 gram for profile length 100 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter8GramProfileLength100 = unknownDocumentForCharacter8Gram[:100]\n",
    "        knownDocumentCfdForCharacter8GramProfileLength100 = knownDocumentForCharacter8Gram[:100]\n",
    "        dissimilarityCharacter8GramProfileLength100 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter8GramProfileLength100,knownDocumentCfdForCharacter8GramProfileLength100)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 8 gram for profile length 100 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Word 1 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForWord1GramProfileLength200 = unknownDocumentForWord1Gram[:200]\n",
    "        knownDocumentCfdForWord1GramProfileLength200 = knownDocumentForWord1Gram[:200]\n",
    "        dissimilarityForWord1GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForWord1GramProfileLength200,knownDocumentCfdForWord1GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Word 1 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Word 2 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForWord2GramProfileLength200 = unknownDocumentForWord2Gram[:200]\n",
    "        knownDocumentCfdForWord2GramProfileLength200 = knownDocumentForWord2Gram[:200]\n",
    "        dissimilarityForWord2GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForWord2GramProfileLength200,knownDocumentCfdForWord2GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Word 2 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Word 3 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForWord3GramProfileLength200 = unknownDocumentForWord3Gram[:200]\n",
    "        knownDocumentCfdForWord3GramProfileLength200 = knownDocumentForWord3Gram[:200]\n",
    "        dissimilarityForWord3GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForWord3GramProfileLength200,knownDocumentCfdForWord3GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Word 3 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 4 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter4GramProfileLength200 = unknownDocumentForCharacter4Gram[:200]\n",
    "        knownDocumentCfdForCharacter4GramProfileLength200 = knownDocumentForCharacter4Gram[:200]\n",
    "        dissimilarityCharacter4GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter4GramProfileLength200,knownDocumentCfdForCharacter4GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 4 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 5 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter5GramProfileLength200 = unknownDocumentForCharacter5Gram[:200]\n",
    "        knownDocumentCfdForCharacter5GramProfileLength200 = knownDocumentForCharacter5Gram[:200]\n",
    "        dissimilarityCharacter5GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter5GramProfileLength200,knownDocumentCfdForCharacter5GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 5 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 6 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter6GramProfileLength200 = unknownDocumentForCharacter6Gram[:200]\n",
    "        knownDocumentCfdForCharacter6GramProfileLength200 = knownDocumentForCharacter6Gram[:200]\n",
    "        dissimilarityCharacter6GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter6GramProfileLength200,knownDocumentCfdForCharacter6GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 6 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 7 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter7GramProfileLength200 = unknownDocumentForCharacter7Gram[:200]\n",
    "        knownDocumentCfdForCharacter7GramProfileLength200 = knownDocumentForCharacter7Gram[:200]\n",
    "        dissimilarityCharacter7GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter7GramProfileLength200,knownDocumentCfdForCharacter7GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 7 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 8 gram for profile length 200 Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentCfdForCharacter8GramProfileLength200 = unknownDocumentForCharacter8Gram[:200]\n",
    "        knownDocumentCfdForCharacter8GramProfileLength200 = knownDocumentForCharacter8Gram[:200]\n",
    "        dissimilarityCharacter8GramProfileLength200 = DissimilarityAlgorithm(unknownDocumentCfdForCharacter8GramProfileLength200,knownDocumentCfdForCharacter8GramProfileLength200)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For Character 8 gram for profile length 200 Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For most common POS tags Starts\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        unknownDocumentMostCommonPosTags = EvaluatePosTagCount(unknownDocumentTagset)\n",
    "        knownDocumentMostCommonPosTags = EvaluatePosTagCount(knownDocumentTagset)\n",
    "        dissimilarityPosTagCount = DissimilarityAlgorithm(unknownDocumentMostCommonPosTags, knownDocumentMostCommonPosTags)\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        # For  most common POS tags Ends\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        tempDissimilarityValues = (json.loads(item)['id'], dissimilarityForPosTagVerb\n",
    "                                   ,dissimilarityForPosTagNoun,dissimilarityForPosTagPronoun,\n",
    "                                   dissimilarityForPosTagAdjective,dissimilarityForWord1GramProfileLength100,\n",
    "                                   dissimilarityForWord2GramProfileLength100,dissimilarityForWord3GramProfileLength100,\n",
    "                                   dissimilarityCharacter4GramProfileLength100,dissimilarityCharacter5GramProfileLength100,\n",
    "                                   dissimilarityCharacter6GramProfileLength100,dissimilarityCharacter7GramProfileLength100,\n",
    "                                   dissimilarityCharacter8GramProfileLength100,dissimilarityForWord1GramProfileLength200,\n",
    "                                   dissimilarityForWord2GramProfileLength200,dissimilarityForWord3GramProfileLength200,\n",
    "                                   dissimilarityCharacter4GramProfileLength200,dissimilarityCharacter5GramProfileLength200,\n",
    "                                   dissimilarityCharacter6GramProfileLength200,dissimilarityCharacter7GramProfileLength200,\n",
    "                                   dissimilarityCharacter8GramProfileLength200,dissimilarityPosTagCount)\n",
    "\n",
    "        dissimilarityValues.append(tempDissimilarityValues)\n",
    "    \n",
    "    # DataFrame to add the list\n",
    "    pairsDataFrame = pd.DataFrame(dissimilarityValues, columns=['id', 'PosTagVerbValue'\n",
    "                                                                ,'PosTagNounValue','PosTagPronounValue'\n",
    "                                                                ,'PosTagAdjectiveValue','Word1Gram100Value'\n",
    "                                                                ,'Word2Gram100Value','Word3Gram100Value'\n",
    "                                                                ,'Character4Gram100Value','Character5Gram100Value'\n",
    "                                                                ,'Character6Gram100Value','Character7Gram100Value'\n",
    "                                                                ,'Character8Gram100Value','Word1Gram200Value'\n",
    "                                                                ,'Word2Gram200Value','Word3Gram200Value'\n",
    "                                                                ,'Character4Gram200Value','Character5Gram200Value'\n",
    "                                                                ,'Character6Gram200Value','Character7Gram200Value'\n",
    "                                                                ,'Character8Gram200Value','PosTagCountValue'])     \n",
    "    \n",
    "\n",
    "    GroundTruthDataJsonList=[]        \n",
    "    \n",
    "    for item in GroundTruthDataList:\n",
    "        tempGroundTruthData = (json.loads(item)['id'], json.loads(item)['same'])\n",
    "        GroundTruthDataJsonList.append(tempGroundTruthData)\n",
    "    GroundTruthDataFrame=pd.DataFrame(GroundTruthDataJsonList, columns=['id','same'])    \n",
    "    completeDataFrame = pd.merge(pairsDataFrame, GroundTruthDataFrame, on='id')\n",
    "     \n",
    "    return completeDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function transform unnormalised dissimilarity values to normalised similarity value\n",
    "# For normalisation, min-max normalisation is used\n",
    "# And this function returns Json List which will be used to generate ansers.jsonl file\n",
    "def DataTransformation(InputDataFrame,ColumnToBeNormalised):    \n",
    "    # Tuple to Dataframe\n",
    "    dissimilarityValues = InputDataFrame[[ColumnToBeNormalised]].values.astype(float) #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    normalisedDissimilarityValues = min_max_scaler.fit_transform(dissimilarityValues)\n",
    "    normalisedDataFrame=pd.DataFrame(normalisedDissimilarityValues,columns=['NormalisedValue'])\n",
    "    normalisedDataFrame['id']= pd.DataFrame(InputDataFrame['id'])\n",
    "    normalisedDataFrame['SimilarityValue'] = 1-pd.DataFrame(normalisedDissimilarityValues) \n",
    "    # The below code is for dicretisation of columns after normalisation\n",
    "    # normalisedDataFrame['NewSimilarityValue'] = normalisedDataFrame['SimilarityValue']   \n",
    "    # normalisedDataFrame.loc[(normalisedDataFrame['SimilarityValue'] < 0.4),'NewSimilarityValue'] = 0   \n",
    "    # normalisedDataFrame.loc[((normalisedDataFrame['SimilarityValue'] >= 0.4) \n",
    "    #                          & (normalisedDataFrame['SimilarityValue'] <= 0.6)),'NewSimilarityValue'] = 0.5 \n",
    "    # normalisedDataFrame.loc[(normalisedDataFrame['SimilarityValue'] > 0.6),'NewSimilarityValue'] = 1 \n",
    "    newIndex = ['id', 'NormalisedValue', 'SimilarityValue']\n",
    "    # newIndex = ['id', 'NormalisedValue', 'NewSimilarityValue']\n",
    "    normalisedDataFrame=normalisedDataFrame.reindex(columns=newIndex)    \n",
    "    normalizedJsonList=[]\n",
    "    for row in normalisedDataFrame.itertuples():\n",
    "        temporaryTuple = {}\n",
    "        temporaryTuple['id'] = row[1]\n",
    "        temporaryTuple['value'] =row[3]\n",
    "        normalizedJsonList.append(temporaryTuple)\n",
    "    return normalizedJsonList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate groundtruth in Json list which will be \n",
    "# later be used to used to generate truth.jsonl\n",
    "# Only used when part of training or test data is used\n",
    "def GenerateTruth(InputDataframe):    \n",
    "    # Tuple to Dataframe        \n",
    "    normalizedJsonList=[]\n",
    "    for row in InputDataframe.itertuples():\n",
    "        temporaryTuple = {}\n",
    "        temporaryTuple['id'] = row[1]\n",
    "        temporaryTuple['same'] =row[2]\n",
    "        normalizedJsonList.append(temporaryTuple)\n",
    "    return normalizedJsonList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSONL file depending upon data.\n",
    "# Input JSON List\n",
    "# Output JSONL file\n",
    "def CreateJSONLFiles(folderPath,fileName, data):\n",
    "    if not os.path.exists(folderPath):\n",
    "        os.makedirs(folderPath)\n",
    "    with open(folderPath+'/'+fileName, 'w') as outfile:\n",
    "        for entry in data:\n",
    "            json.dump(entry,outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Path to be set\n",
    "RootPath='./Datasets'\n",
    "Path=os.path.join(RootPath, 'pan20-authorship-verification','DissimilarityMethod')\n",
    "InputPathForTruthTraining=os.path.join(Path, 'Output','Training')\n",
    "InputPathForTruthValidation=os.path.join(Path, 'Output','Validation')\n",
    "InputPathForTruthTest=os.path.join(Path, 'Output','Test')\n",
    "InputPathForTruthAll=os.path.join(Path, 'Output','All')\n",
    "\n",
    "# For Training \n",
    "class EvaluatorVerbArgsTraining:    \n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Verb','Training')    \n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorNounArgsTraining:\n",
    "    commonPath= os.path.join(Path, 'Output','PosTag','Noun','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorAdjectiveArgsTraining:\n",
    "    commonPath= os.path.join(Path, 'Output','PosTag','Adjective','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath  \n",
    "class EvaluatorPronounArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Pronoun','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord1GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength100ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength100','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorWord1GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength200ArgsTraining:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength200','Training')\n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath \n",
    "class EvaluatorMostCommonPosTagCountArgsTraining:    \n",
    "    commonPath=os.path.join(Path, 'Output','PosTagCount','Training')   \n",
    "    i = InputPathForTruthTraining\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "    \n",
    "    \n",
    "# For Validation   \n",
    "class EvaluatorVerbArgsValidation:\n",
    "    commonPath= os.path.join(Path, 'Output','Verb','Validation')   \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorNounArgsValidation:\n",
    "    commonPath= os.path.join(Path, 'Output','Noun','Validation')   \n",
    "    i =InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorAdjectiveArgsValidation:\n",
    "    commonPath= os.path.join(Path, 'Output','Adjective','Validation')   \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorPronounArgsValidation:\n",
    "    commonPath= os.path.join(Path, 'Output','Pronoun','Validation')   \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord1GramProfileLength100ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength100','Validation') \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength100ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength100','Validation') \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength100ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength100','Validation') \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength100ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength100','Validation') \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength100ArgsValidation:\n",
    "    commonPath=commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength100','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength100ArgsValidation:\n",
    "    commonPath=commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength100','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength100ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength100','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength100ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength100','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorWord1GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength200','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength200','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength200','Validation')\n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength200','Validation')    \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength200','Validation')    \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength200','Validation')    \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength200','Validation')    \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength200ArgsValidation:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength200','Validation')    \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath     \n",
    "class EvaluatorMostCommonPosTagCountArgsValidation:   \n",
    "    commonPath=os.path.join(Path, 'Output','PosTagCount','Validation')      \n",
    "    i = InputPathForTruthValidation\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "    \n",
    "    \n",
    "# For Test   \n",
    "class EvaluatorVerbArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Verb','Test') \n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorNounArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Noun','Test')     \n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorAdjectiveArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Adjective','Test')     \n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorPronounArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Pronoun','Test')     \n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord1GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength100ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength100','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorWord1GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength200ArgsTest:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength200','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorMostCommonPosTagCountArgsTest:    \n",
    "    commonPath=os.path.join(Path, 'Output','PosTagCount','Test')\n",
    "    i = InputPathForTruthTest\n",
    "    a = commonPath\n",
    "    o = commonPath   \n",
    "\n",
    "class EvaluatorAllArgsAll:    \n",
    "    commonPath=Path + '/Output/All/Test'    \n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath   \n",
    "    \n",
    "    \n",
    "# For All   \n",
    "class EvaluatorVerbArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Verb','All') \n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorNounArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Noun','All')     \n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorAdjectiveArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Adjective','All')     \n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorPronounArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','PosTag','Pronoun','All')     \n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord1GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength100ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength100','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorWord1GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Word1Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord2GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Word2Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorWord3GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Word3Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter4GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character4Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter5GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character5Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter6GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character6Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath\n",
    "class EvaluatorCharacter7GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character7Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorCharacter8GramProfileLength200ArgsAll:\n",
    "    commonPath=os.path.join(Path, 'Output','Character8Gram','ProfileLength200','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath    \n",
    "class EvaluatorMostCommonPosTagCountArgsAll:    \n",
    "    commonPath=os.path.join(Path, 'Output','PosTagCount','All')\n",
    "    i = InputPathForTruthAll\n",
    "    a = commonPath\n",
    "    o = commonPath   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for different type of Feature for either training, validation or test phase\n",
    "def EvaluationTypeForDifferentFeatures(InputDataFrame,Feature,Type):  \n",
    "    if (Type=='Training'):\n",
    "        # For Training\n",
    "        if(Feature=='PosTagVerbValue'):\n",
    "            CreateJSONLFiles(EvaluatorVerbArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature)) \n",
    "            evaluator.main(EvaluatorVerbArgsTraining)\n",
    "        elif(Feature=='PosTagNounValue'):\n",
    "            CreateJSONLFiles(EvaluatorNounArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorNounArgsTraining)\n",
    "        elif(Feature=='PosTagPronounValue'):\n",
    "            CreateJSONLFiles(EvaluatorPronounArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorPronounArgsTraining)\n",
    "        elif(Feature=='PosTagAdjectiveValue'):\n",
    "            CreateJSONLFiles(EvaluatorAdjectiveArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorAdjectiveArgsTraining)        \n",
    "        elif(Feature=='Word1Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength100ArgsTraining)\n",
    "        elif(Feature=='Word2Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength100ArgsTraining)\n",
    "        elif(Feature=='Word3Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength100ArgsTraining)\n",
    "        if(Feature=='Character4Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature)) \n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength100ArgsTraining)\n",
    "        elif(Feature=='Character5Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength100ArgsTraining)\n",
    "        elif(Feature=='Character6Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength100ArgsTraining)\n",
    "        elif(Feature=='Character7Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength100ArgsTraining)        \n",
    "        elif(Feature=='Character8Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength100ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength100ArgsTraining)            \n",
    "        elif(Feature=='Word1Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength200ArgsTraining)\n",
    "        elif(Feature=='Word2Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength200ArgsTraining)\n",
    "        elif(Feature=='Word3Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength200ArgsTraining)\n",
    "        elif(Feature=='Character4Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength200ArgsTraining)        \n",
    "        elif(Feature=='Character5Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength200ArgsTraining)\n",
    "        elif(Feature=='Character6Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength200ArgsTraining)\n",
    "        elif(Feature=='Character7Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength200ArgsTraining)\n",
    "        elif(Feature=='Character8Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength200ArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength200ArgsTraining)    \n",
    "        elif(Feature=='PosTagCountValue'):\n",
    "            CreateJSONLFiles(EvaluatorMostCommonPosTagCountArgsTraining.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorMostCommonPosTagCountArgsTraining) \n",
    "    elif(Type=='Validation'):\n",
    "        # For Validation\n",
    "        if(Feature=='PosTagVerbValue'):\n",
    "            CreateJSONLFiles(EvaluatorVerbArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorVerbArgsValidation)\n",
    "        elif(Feature=='PosTagNounValue'):\n",
    "            CreateJSONLFiles(EvaluatorNounArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorNounArgsValidation)\n",
    "        elif(Feature=='PosTagPronounValue'):\n",
    "            CreateJSONLFiles(EvaluatorPronounArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorPronounArgsValidation)\n",
    "        elif(Feature=='PosTagAdjectiveValue'):\n",
    "            CreateJSONLFiles(EvaluatorAdjectiveArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorAdjectiveArgsValidation)            \n",
    "        elif(Feature=='Word1Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength100ArgsValidation)\n",
    "        elif(Feature=='Word2Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength100ArgsValidation)\n",
    "        elif(Feature=='Word3Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength100ArgsValidation)\n",
    "        if(Feature=='Character4Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature)) \n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength100ArgsValidation)\n",
    "        elif(Feature=='Character5Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength100ArgsValidation)\n",
    "        elif(Feature=='Character6Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength100ArgsValidation)\n",
    "        elif(Feature=='Character7Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength100ArgsValidation)        \n",
    "        elif(Feature=='Character8Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength100ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength100ArgsValidation)            \n",
    "        elif(Feature=='Word1Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength200ArgsValidation)\n",
    "        elif(Feature=='Word2Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength200ArgsValidation)\n",
    "        elif(Feature=='Word3Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength200ArgsValidation)\n",
    "        elif(Feature=='Character4Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength200ArgsValidation)        \n",
    "        elif(Feature=='Character5Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength200ArgsValidation)\n",
    "        elif(Feature=='Character6Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength200ArgsValidation)\n",
    "        elif(Feature=='Character7Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength200ArgsValidation)\n",
    "        elif(Feature=='Character8Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength200ArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength200ArgsValidation)    \n",
    "        elif(Feature=='PosTagCountValue'):\n",
    "            CreateJSONLFiles(EvaluatorMostCommonPosTagCountArgsValidation.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorMostCommonPosTagCountArgsValidation)            \n",
    "    elif(Type=='Test'):\n",
    "        # For Test\n",
    "        if(Feature=='PosTagVerbValue'):\n",
    "            CreateJSONLFiles(EvaluatorVerbArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorVerbArgsTest)\n",
    "        elif(Feature=='PosTagNounValue'):\n",
    "            CreateJSONLFiles(EvaluatorNounArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorNounArgsTest)\n",
    "        elif(Feature=='PosTagPronounValue'):\n",
    "            CreateJSONLFiles(EvaluatorPronounArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorPronounArgsTest)\n",
    "        elif(Feature=='PosTagAdjectiveValue'):\n",
    "            CreateJSONLFiles(EvaluatorAdjectiveArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorAdjectiveArgsTest)            \n",
    "        elif(Feature=='Word1Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength100ArgsTest)\n",
    "        elif(Feature=='Word2Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength100ArgsTest)\n",
    "        elif(Feature=='Word3Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength100ArgsTest)\n",
    "        if(Feature=='Character4Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature)) \n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength100ArgsTest)\n",
    "        elif(Feature=='Character5Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength100ArgsTest)\n",
    "        elif(Feature=='Character6Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength100ArgsTest)\n",
    "        elif(Feature=='Character7Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength100ArgsTest)        \n",
    "        elif(Feature=='Character8Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength100ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength100ArgsTest)            \n",
    "        elif(Feature=='Word1Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Word2Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Word3Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Character4Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength200ArgsTest)        \n",
    "        elif(Feature=='Character5Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Character6Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Character7Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Character8Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength200ArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength200ArgsTest)    \n",
    "        elif(Feature=='PosTagCountValue'):\n",
    "            CreateJSONLFiles(EvaluatorMostCommonPosTagCountArgsTest.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorMostCommonPosTagCountArgsTest)\n",
    "    elif(Type=='All'): #To Do\n",
    "        # For Test\n",
    "        if(Feature=='PosTagVerbValue'):\n",
    "            CreateJSONLFiles(EvaluatorVerbArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorVerbArgsAll)\n",
    "        elif(Feature=='PosTagNounValue'):\n",
    "            CreateJSONLFiles(EvaluatorNounArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorNounArgsAll)\n",
    "        elif(Feature=='PosTagPronounValue'):\n",
    "            CreateJSONLFiles(EvaluatorPronounArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorPronounArgsAll)\n",
    "        elif(Feature=='PosTagAdjectiveValue'):\n",
    "            CreateJSONLFiles(EvaluatorAdjectiveArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorAdjectiveArgsAll)            \n",
    "        elif(Feature=='Word1Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength100ArgsAll)\n",
    "        elif(Feature=='Word2Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength100ArgsAll)\n",
    "        elif(Feature=='Word3Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength100ArgsAll)\n",
    "        if(Feature=='Character4Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature)) \n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength100ArgsAll)\n",
    "        elif(Feature=='Character5Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength100ArgsTest)\n",
    "        elif(Feature=='Character6Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength100ArgsAll)\n",
    "        elif(Feature=='Character7Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength100ArgsTest)        \n",
    "        elif(Feature=='Character8Gram100Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength100ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength100ArgsAll)            \n",
    "        elif(Feature=='Word1Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord1GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord1GramProfileLength200ArgsAll)\n",
    "        elif(Feature=='Word2Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord2GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord2GramProfileLength200ArgsAll)\n",
    "        elif(Feature=='Word3Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorWord3GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorWord3GramProfileLength200ArgsAll)\n",
    "        elif(Feature=='Character4Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter4GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter4GramProfileLength200ArgsAll)        \n",
    "        elif(Feature=='Character5Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter5GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter5GramProfileLength200ArgsAll)\n",
    "        elif(Feature=='Character6Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter6GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter6GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Character7Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter7GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter7GramProfileLength200ArgsTest)\n",
    "        elif(Feature=='Character8Gram200Value'):\n",
    "            CreateJSONLFiles(EvaluatorCharacter8GramProfileLength200ArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorCharacter8GramProfileLength200ArgsAll)    \n",
    "        elif(Feature=='PosTagCountValue'):\n",
    "            CreateJSONLFiles(EvaluatorMostCommonPosTagCountArgsAll.a,'answers.jsonl',DataTransformation(InputDataFrame,Feature))\n",
    "            evaluator.main(EvaluatorMostCommonPosTagCountArgsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Dissimilarity Algorithm for different length of datapoints\n",
    "# If Length is 0, then full dataset would be taken into consideration, \n",
    "# else specified length would be taken into consideration for training phase \n",
    "# with 10% of length validation phase and test phase\n",
    "\n",
    "def EvaluationOfDissimilarityAlgorithm(Length):\n",
    "    \n",
    "    if ((int(Length))==0):\n",
    "        dissimilarityTrainingDataFrame=EvaluateDissimilaritiesFullDocumentSecondProcedure(TrainingJsonlist,GroundTruthTrainingJsonlist)   \n",
    "        \n",
    "        \n",
    "        dissimilarityValidationDataFrame=EvaluateDissimilaritiesFullDocumentSecondProcedure(ValidationJsonlist,GroundTruthValidationJsonlist)\n",
    "        dissimilarityTestDataFrame=EvaluateDissimilaritiesFullDocumentSecondProcedure(TestJsonlist,GroundTruthTestJsonlist) \n",
    "        \n",
    "    else:\n",
    "        tempLength=int(Length/2)        \n",
    "        newLength=23670+tempLength+1\n",
    "        testLength=int(0.1*Length)+1\n",
    "        \n",
    "        dissimilarityTrainingDataFrame=EvaluateDissimilaritiesFullDocumentSecondProcedure(TrainingJsonlist[0:tempLength]+TrainingJsonlist[23670:newLength]\n",
    "                                                                                                     ,GroundTruthTrainingJsonlist[0:tempLength]+GroundTruthTrainingJsonlist[23670:newLength])\n",
    "        dissimilarityValidationDataFrame=EvaluateDissimilaritiesFullDocumentSecondProcedure(ValidationJsonlist[0:testLength],GroundTruthValidationJsonlist[0:testLength])       \n",
    "        dissimilarityTestDataFrame=EvaluateDissimilaritiesFullDocumentSecondProcedure(TestJsonlist[0:testLength],GroundTruthTestJsonlist[0:testLength]) \n",
    "        \n",
    "    # Training Dataset    \n",
    "    groundTruthTrainingDataFrame = dissimilarityTrainingDataFrame[['id', 'same']]\n",
    "    CreateJSONLFiles(InputPathForTruthTraining,'truth.jsonl',GenerateTruth(groundTruthTrainingDataFrame))\n",
    "    # Validation Dataset\n",
    "    groundTruthValidationDataFrame = dissimilarityValidationDataFrame[['id', 'same']]\n",
    "    CreateJSONLFiles(InputPathForTruthValidation,'truth.jsonl',GenerateTruth(groundTruthValidationDataFrame))\n",
    "    # Test Dataset\n",
    "    groundTruthTestDataFrame = dissimilarityTestDataFrame[['id', 'same']]\n",
    "    CreateJSONLFiles(InputPathForTruthTest,'truth.jsonl',GenerateTruth(groundTruthTestDataFrame))  \n",
    "    \n",
    "    # Combining Dataset\n",
    "    groundTruthAllDataFrame = pd.concat([groundTruthTrainingDataFrame, groundTruthValidationDataFrame], ignore_index=True)\n",
    "    groundTruthAllDataFrame = pd.concat([groundTruthAllDataFrame, groundTruthTestDataFrame], ignore_index=True)\n",
    "    CreateJSONLFiles(InputPathForTruthAll,'truth.jsonl',GenerateTruth(groundTruthAllDataFrame))\n",
    "    \n",
    "    dissimilarityAllDataFrame=pd.concat([dissimilarityTrainingDataFrame, dissimilarityValidationDataFrame], ignore_index=True)\n",
    "    dissimilarityAllDataFrame=pd.concat([dissimilarityTrainingDataFrame, dissimilarityTestDataFrame], ignore_index=True)\n",
    "    \n",
    "    tempDataFrame=pd.DataFrame(dissimilarityTrainingDataFrame.loc[:, dissimilarityTrainingDataFrame.columns != 'id'])\n",
    "    columnsOfDissimilarityDataFrame=pd.DataFrame(tempDataFrame.loc[:, tempDataFrame.columns != 'same']).columns\n",
    "    columnsOfDissimilarityDataFrame\n",
    "    \n",
    "    for column in columnsOfDissimilarityDataFrame:\n",
    "        print('Evaluation for ' + column +'  for Training Data Starts')\n",
    "        EvaluationTypeForDifferentFeatures(dissimilarityTrainingDataFrame,column,'Training')\n",
    "        print('Evaluation for ' + column +'  for Training Data Ends')\n",
    "        print('Evaluation for ' + column +'  for Validation Data Starts')\n",
    "        EvaluationTypeForDifferentFeatures(dissimilarityValidationDataFrame,column,'Validation')\n",
    "        print('Evaluation for ' + column +'  for Validation Data Ends')\n",
    "        print('Evaluation for ' + column +'  for Test Data Starts')\n",
    "        EvaluationTypeForDifferentFeatures(dissimilarityTestDataFrame,column,'Test')\n",
    "        print('Evaluation for ' + column +'  for Test Data Ends')\n",
    "        print('Evaluation for ' + column +'  for All Data Starts')\n",
    "        EvaluationTypeForDifferentFeatures(dissimilarityAllDataFrame,column,'All')\n",
    "        print('Evaluation for ' + column +'  for All Data Ends')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of data local path\n",
    "TrainingJsonlist=LoadAllData(os.path.join(Path, 'training','pairs.jsonl'))\n",
    "GroundTruthTrainingJsonlist=LoadAllData(os.path.join(Path, 'training','truth.jsonl'))\n",
    "ValidationJsonlist=LoadAllData(os.path.join(Path, 'validation','pairs.jsonl'))\n",
    "GroundTruthValidationJsonlist=LoadAllData(os.path.join(Path, 'validation','truth.jsonl'))\n",
    "TestJsonlist=LoadAllData(os.path.join(Path, 'test','pairs.jsonl'))\n",
    "GroundTruthTestJsonlist=LoadAllData(os.path.join(Path, 'test','truth.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Point of the Program\n",
    "def main(): \n",
    "    # print('Test for 1000 training datapoints Starts: ',datetime.now())\n",
    "    # startAlgorithmAndEvaluation=perf_counter()\n",
    "    # EvaluationOfDissimilarityAlgorithm(1000)\n",
    "    # durationOfAlgorithmAndEvaluation=perf_counter() - startAlgorithmAndEvaluation\n",
    "    # print('Time taken: '+ format(durationOfAlgorithmAndEvaluation))\n",
    "    # print('Test for 1000 training datapoints Ends: ',datetime.now())\n",
    "    print('Evaluation for whole dataset Starts: ',datetime.now())\n",
    "    startAlgorithmAndEvaluation=perf_counter()\n",
    "    EvaluationOfDissimilarityAlgorithm(0)\n",
    "    durationOfAlgorithmAndEvaluation=perf_counter() - startAlgorithmAndEvaluation\n",
    "    print('Time taken: '+ format(durationOfAlgorithmAndEvaluation))\n",
    "    print('Evaluation for whole dataset Ends: ',datetime.now())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the above cells and this cell only. Do not run the cells below\n",
    "# For whole dataset without discretization -without range - For all features\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 100 training datapoints Starts\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.599, 'c@1': 0.531, 'f_05_u': 0.522, 'F1': 0.667, 'overall': 0.58}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.397, 'f_05_u': 0.781, 'F1': 0.4, 'overall': 0.561}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.929, 'c@1': 0.893, 'f_05_u': 0.714, 'F1': 0.889, 'overall': 0.856}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.584, 'c@1': 0.506, 'f_05_u': 0.538, 'F1': 0.673, 'overall': 0.575}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.537, 'c@1': 0.477, 'f_05_u': 0.498, 'F1': 0.481, 'overall': 0.498}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.646, 'c@1': 0.579, 'f_05_u': 0.833, 'F1': 0.667, 'overall': 0.681}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.536, 'c@1': 0.545, 'f_05_u': 0.417, 'F1': 0.444, 'overall': 0.486}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.543, 'c@1': 0.46, 'f_05_u': 0.533, 'F1': 0.484, 'overall': 0.505}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.738, 'c@1': 0.683, 'f_05_u': 0.603, 'F1': 0.771, 'overall': 0.699}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.812, 'c@1': 0.579, 'f_05_u': 0.893, 'F1': 0.571, 'overall': 0.714}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.857, 'c@1': 0.744, 'f_05_u': 0.556, 'F1': 0.857, 'overall': 0.753}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.724, 'c@1': 0.64, 'f_05_u': 0.614, 'F1': 0.773, 'overall': 0.688}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.772, 'c@1': 0.732, 'f_05_u': 0.653, 'F1': 0.788, 'overall': 0.736}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.938, 'c@1': 0.562, 'f_05_u': 0.972, 'F1': 0.667, 'overall': 0.785}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.946, 'c@1': 0.868, 'f_05_u': 0.625, 'F1': 1.0, 'overall': 0.86}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.779, 'c@1': 0.717, 'f_05_u': 0.674, 'F1': 0.806, 'overall': 0.744}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.727, 'c@1': 0.718, 'f_05_u': 0.673, 'F1': 0.667, 'overall': 0.696}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.688, 'c@1': 0.43, 'f_05_u': 0.75, 'F1': 0.286, 'overall': 0.538}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.982, 'c@1': 0.967, 'f_05_u': 0.833, 'F1': 1.0, 'overall': 0.946}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.753, 'c@1': 0.716, 'f_05_u': 0.702, 'F1': 0.697, 'overall': 0.717}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.668, 'c@1': 0.582, 'f_05_u': 0.581, 'F1': 0.734, 'overall': 0.641}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.625, 'c@1': 0.645, 'f_05_u': 0.75, 'F1': 0.769, 'overall': 0.697}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.554, 'c@1': 0.496, 'f_05_u': 0.469, 'F1': 0.4, 'overall': 0.48}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.635, 'c@1': 0.535, 'f_05_u': 0.59, 'F1': 0.721, 'overall': 0.62}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.705, 'c@1': 0.639, 'f_05_u': 0.592, 'F1': 0.75, 'overall': 0.671}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.694, 'f_05_u': 0.833, 'F1': 0.8, 'overall': 0.78}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.589, 'c@1': 0.496, 'f_05_u': 0.417, 'F1': 0.571, 'overall': 0.518}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.589, 'c@1': 0.496, 'f_05_u': 0.417, 'F1': 0.571, 'overall': 0.518}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.665, 'c@1': 0.582, 'f_05_u': 0.548, 'F1': 0.725, 'overall': 0.63}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.708, 'c@1': 0.579, 'f_05_u': 0.781, 'F1': 0.667, 'overall': 0.684}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.929, 'c@1': 0.744, 'f_05_u': 0.5, 'F1': 0.889, 'overall': 0.765}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.652, 'c@1': 0.559, 'f_05_u': 0.564, 'F1': 0.733, 'overall': 0.627}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.78, 'c@1': 0.728, 'f_05_u': 0.661, 'F1': 0.8, 'overall': 0.742}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.661, 'f_05_u': 0.795, 'F1': 0.889, 'overall': 0.779}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.967, 'f_05_u': 0.714, 'F1': 1.0, 'overall': 0.92}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.783, 'c@1': 0.705, 'f_05_u': 0.68, 'F1': 0.812, 'overall': 0.745}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.74, 'c@1': 0.718, 'f_05_u': 0.738, 'F1': 0.565, 'overall': 0.69}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.688, 'c@1': 0.496, 'f_05_u': 0.75, 'F1': 0.444, 'overall': 0.594}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.992, 'f_05_u': 0.833, 'F1': 1.0, 'overall': 0.956}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.71, 'f_05_u': 0.765, 'F1': 0.583, 'overall': 0.707}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.692, 'c@1': 0.615, 'f_05_u': 0.554, 'F1': 0.742, 'overall': 0.651}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.694, 'f_05_u': 0.75, 'F1': 0.8, 'overall': 0.728}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.661, 'c@1': 0.62, 'f_05_u': 0.536, 'F1': 0.5, 'overall': 0.579}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.671, 'c@1': 0.585, 'f_05_u': 0.573, 'F1': 0.745, 'overall': 0.644}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.691, 'c@1': 0.608, 'f_05_u': 0.561, 'F1': 0.747, 'overall': 0.652}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.793, 'f_05_u': 0.938, 'F1': 0.833, 'overall': 0.86}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.607, 'c@1': 0.496, 'f_05_u': 0.5, 'F1': 0.571, 'overall': 0.544}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.664, 'c@1': 0.573, 'f_05_u': 0.579, 'F1': 0.742, 'overall': 0.64}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.531, 'c@1': 0.494, 'f_05_u': 0.525, 'F1': 0.637, 'overall': 0.547}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.646, 'c@1': 0.636, 'f_05_u': 0.781, 'F1': 0.714, 'overall': 0.694}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.643, 'c@1': 0.496, 'f_05_u': 0.455, 'F1': 0.615, 'overall': 0.552}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.495, 'c@1': 0.478, 'f_05_u': 0.539, 'F1': 0.631, 'overall': 0.536}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 469.8339286595583\n",
      "Test for 100 training datapoints Ends\n",
      "Test for 100 training datapoints Starts\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.599, 'c@1': 0.531, 'f_05_u': 0.522, 'F1': 0.667, 'overall': 0.58}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.397, 'f_05_u': 0.781, 'F1': 0.4, 'overall': 0.561}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.929, 'c@1': 0.893, 'f_05_u': 0.714, 'F1': 0.889, 'overall': 0.856}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.584, 'c@1': 0.506, 'f_05_u': 0.538, 'F1': 0.673, 'overall': 0.575}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.537, 'c@1': 0.477, 'f_05_u': 0.498, 'F1': 0.481, 'overall': 0.498}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.646, 'c@1': 0.579, 'f_05_u': 0.833, 'F1': 0.667, 'overall': 0.681}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.536, 'c@1': 0.545, 'f_05_u': 0.417, 'F1': 0.444, 'overall': 0.486}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.543, 'c@1': 0.46, 'f_05_u': 0.533, 'F1': 0.484, 'overall': 0.505}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.738, 'c@1': 0.683, 'f_05_u': 0.603, 'F1': 0.771, 'overall': 0.699}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.812, 'c@1': 0.579, 'f_05_u': 0.893, 'F1': 0.571, 'overall': 0.714}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.857, 'c@1': 0.744, 'f_05_u': 0.556, 'F1': 0.857, 'overall': 0.753}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.724, 'c@1': 0.64, 'f_05_u': 0.614, 'F1': 0.773, 'overall': 0.688}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.772, 'c@1': 0.732, 'f_05_u': 0.653, 'F1': 0.788, 'overall': 0.736}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.938, 'c@1': 0.562, 'f_05_u': 0.972, 'F1': 0.667, 'overall': 0.785}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.946, 'c@1': 0.868, 'f_05_u': 0.625, 'F1': 1.0, 'overall': 0.86}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.779, 'c@1': 0.717, 'f_05_u': 0.674, 'F1': 0.806, 'overall': 0.744}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.727, 'c@1': 0.718, 'f_05_u': 0.673, 'F1': 0.667, 'overall': 0.696}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.688, 'c@1': 0.43, 'f_05_u': 0.75, 'F1': 0.286, 'overall': 0.538}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.982, 'c@1': 0.967, 'f_05_u': 0.833, 'F1': 1.0, 'overall': 0.946}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.753, 'c@1': 0.716, 'f_05_u': 0.702, 'F1': 0.697, 'overall': 0.717}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.668, 'c@1': 0.582, 'f_05_u': 0.581, 'F1': 0.734, 'overall': 0.641}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.625, 'c@1': 0.645, 'f_05_u': 0.75, 'F1': 0.769, 'overall': 0.697}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.554, 'c@1': 0.496, 'f_05_u': 0.469, 'F1': 0.4, 'overall': 0.48}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.635, 'c@1': 0.535, 'f_05_u': 0.59, 'F1': 0.721, 'overall': 0.62}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.705, 'c@1': 0.639, 'f_05_u': 0.592, 'F1': 0.75, 'overall': 0.671}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.694, 'f_05_u': 0.833, 'F1': 0.8, 'overall': 0.78}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.589, 'c@1': 0.496, 'f_05_u': 0.417, 'F1': 0.571, 'overall': 0.518}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.589, 'c@1': 0.496, 'f_05_u': 0.417, 'F1': 0.571, 'overall': 0.518}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.665, 'c@1': 0.582, 'f_05_u': 0.548, 'F1': 0.725, 'overall': 0.63}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.708, 'c@1': 0.579, 'f_05_u': 0.781, 'F1': 0.667, 'overall': 0.684}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.929, 'c@1': 0.744, 'f_05_u': 0.5, 'F1': 0.889, 'overall': 0.765}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.652, 'c@1': 0.559, 'f_05_u': 0.564, 'F1': 0.733, 'overall': 0.627}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.78, 'c@1': 0.728, 'f_05_u': 0.661, 'F1': 0.8, 'overall': 0.742}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.661, 'f_05_u': 0.795, 'F1': 0.889, 'overall': 0.779}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.967, 'f_05_u': 0.714, 'F1': 1.0, 'overall': 0.92}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.783, 'c@1': 0.705, 'f_05_u': 0.68, 'F1': 0.812, 'overall': 0.745}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.74, 'c@1': 0.718, 'f_05_u': 0.738, 'F1': 0.565, 'overall': 0.69}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.688, 'c@1': 0.496, 'f_05_u': 0.75, 'F1': 0.444, 'overall': 0.594}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.992, 'f_05_u': 0.833, 'F1': 1.0, 'overall': 0.956}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.71, 'f_05_u': 0.765, 'F1': 0.583, 'overall': 0.707}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.692, 'c@1': 0.615, 'f_05_u': 0.554, 'F1': 0.742, 'overall': 0.651}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.694, 'f_05_u': 0.75, 'F1': 0.8, 'overall': 0.728}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.661, 'c@1': 0.62, 'f_05_u': 0.536, 'F1': 0.5, 'overall': 0.579}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.671, 'c@1': 0.585, 'f_05_u': 0.573, 'F1': 0.745, 'overall': 0.644}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.691, 'c@1': 0.608, 'f_05_u': 0.561, 'F1': 0.747, 'overall': 0.652}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.793, 'f_05_u': 0.938, 'F1': 0.833, 'overall': 0.86}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.607, 'c@1': 0.496, 'f_05_u': 0.5, 'F1': 0.571, 'overall': 0.544}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.664, 'c@1': 0.573, 'f_05_u': 0.579, 'F1': 0.742, 'overall': 0.64}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.531, 'c@1': 0.494, 'f_05_u': 0.525, 'F1': 0.637, 'overall': 0.547}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.646, 'c@1': 0.636, 'f_05_u': 0.781, 'F1': 0.714, 'overall': 0.694}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.643, 'c@1': 0.496, 'f_05_u': 0.455, 'F1': 0.615, 'overall': 0.552}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.495, 'c@1': 0.478, 'f_05_u': 0.539, 'F1': 0.631, 'overall': 0.536}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 574.6964407861233\n",
      "Test for 100 training datapoints Ends\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 100 datapoints with discretization\n",
    "main()\n",
    "cProfile.run('main()',filename='DissimilarityAlgorithm.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 100 training datapoints Starts\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.626, 'c@1': 0.531, 'f_05_u': 0.522, 'F1': 0.667, 'overall': 0.586}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.583, 'c@1': 0.397, 'f_05_u': 0.781, 'F1': 0.4, 'overall': 0.54}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.964, 'c@1': 0.893, 'f_05_u': 0.714, 'F1': 0.889, 'overall': 0.865}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.617, 'c@1': 0.506, 'f_05_u': 0.538, 'F1': 0.673, 'overall': 0.584}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.552, 'c@1': 0.477, 'f_05_u': 0.498, 'F1': 0.481, 'overall': 0.502}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.579, 'f_05_u': 0.833, 'F1': 0.667, 'overall': 0.707}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.571, 'c@1': 0.545, 'f_05_u': 0.417, 'F1': 0.444, 'overall': 0.494}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.55, 'c@1': 0.46, 'f_05_u': 0.533, 'F1': 0.484, 'overall': 0.507}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.755, 'c@1': 0.683, 'f_05_u': 0.603, 'F1': 0.771, 'overall': 0.703}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.579, 'f_05_u': 0.893, 'F1': 0.571, 'overall': 0.677}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.744, 'f_05_u': 0.556, 'F1': 0.857, 'overall': 0.758}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.741, 'c@1': 0.64, 'f_05_u': 0.614, 'F1': 0.773, 'overall': 0.692}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.805, 'c@1': 0.732, 'f_05_u': 0.653, 'F1': 0.788, 'overall': 0.744}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.562, 'f_05_u': 0.972, 'F1': 0.667, 'overall': 0.769}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.946, 'c@1': 0.868, 'f_05_u': 0.625, 'F1': 1.0, 'overall': 0.86}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.803, 'c@1': 0.717, 'f_05_u': 0.674, 'F1': 0.806, 'overall': 0.75}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.743, 'c@1': 0.718, 'f_05_u': 0.673, 'F1': 0.667, 'overall': 0.7}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.708, 'c@1': 0.43, 'f_05_u': 0.75, 'F1': 0.286, 'overall': 0.543}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.982, 'c@1': 0.967, 'f_05_u': 0.833, 'F1': 1.0, 'overall': 0.946}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.774, 'c@1': 0.716, 'f_05_u': 0.702, 'F1': 0.697, 'overall': 0.722}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.741, 'c@1': 0.582, 'f_05_u': 0.581, 'F1': 0.734, 'overall': 0.66}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.645, 'f_05_u': 0.75, 'F1': 0.769, 'overall': 0.734}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.571, 'c@1': 0.496, 'f_05_u': 0.469, 'F1': 0.4, 'overall': 0.484}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.691, 'c@1': 0.535, 'f_05_u': 0.59, 'F1': 0.721, 'overall': 0.634}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.734, 'c@1': 0.639, 'f_05_u': 0.592, 'F1': 0.75, 'overall': 0.679}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.708, 'c@1': 0.694, 'f_05_u': 0.833, 'F1': 0.8, 'overall': 0.759}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.589, 'c@1': 0.496, 'f_05_u': 0.417, 'F1': 0.571, 'overall': 0.518}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.589, 'c@1': 0.496, 'f_05_u': 0.417, 'F1': 0.571, 'overall': 0.518}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.712, 'c@1': 0.582, 'f_05_u': 0.548, 'F1': 0.725, 'overall': 0.642}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.625, 'c@1': 0.579, 'f_05_u': 0.781, 'F1': 0.667, 'overall': 0.663}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.744, 'f_05_u': 0.5, 'F1': 0.889, 'overall': 0.783}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.697, 'c@1': 0.559, 'f_05_u': 0.564, 'F1': 0.733, 'overall': 0.638}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.782, 'c@1': 0.728, 'f_05_u': 0.661, 'F1': 0.8, 'overall': 0.743}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.661, 'f_05_u': 0.795, 'F1': 0.889, 'overall': 0.774}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.967, 'f_05_u': 0.714, 'F1': 1.0, 'overall': 0.92}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.785, 'c@1': 0.705, 'f_05_u': 0.68, 'F1': 0.812, 'overall': 0.746}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.745, 'c@1': 0.718, 'f_05_u': 0.738, 'F1': 0.565, 'overall': 0.691}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.542, 'c@1': 0.496, 'f_05_u': 0.75, 'F1': 0.444, 'overall': 0.558}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.992, 'f_05_u': 0.833, 'F1': 1.0, 'overall': 0.956}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.783, 'c@1': 0.71, 'f_05_u': 0.765, 'F1': 0.583, 'overall': 0.71}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.615, 'f_05_u': 0.554, 'F1': 0.742, 'overall': 0.665}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.694, 'f_05_u': 0.75, 'F1': 0.8, 'overall': 0.759}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.679, 'c@1': 0.62, 'f_05_u': 0.536, 'F1': 0.5, 'overall': 0.584}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.713, 'c@1': 0.585, 'f_05_u': 0.573, 'F1': 0.745, 'overall': 0.654}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.749, 'c@1': 0.608, 'f_05_u': 0.561, 'F1': 0.747, 'overall': 0.666}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.793, 'f_05_u': 0.938, 'F1': 0.833, 'overall': 0.839}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.679, 'c@1': 0.496, 'f_05_u': 0.5, 'F1': 0.571, 'overall': 0.561}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.709, 'c@1': 0.573, 'f_05_u': 0.579, 'F1': 0.742, 'overall': 0.651}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.628, 'c@1': 0.494, 'f_05_u': 0.525, 'F1': 0.637, 'overall': 0.571}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.583, 'c@1': 0.636, 'f_05_u': 0.781, 'F1': 0.714, 'overall': 0.679}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.786, 'c@1': 0.496, 'f_05_u': 0.455, 'F1': 0.615, 'overall': 0.588}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.577, 'c@1': 0.478, 'f_05_u': 0.539, 'F1': 0.631, 'overall': 0.556}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 469.150268137455\n",
      "Test for 100 training datapoints Ends\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 100 datapoints without discretization\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 100 training datapoints Starts:  2020-09-05 22:29:39.437911\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.624, 'c@1': 0.554, 'f_05_u': 0.549, 'F1': 0.609, 'overall': 0.584}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.625, 'c@1': 0.636, 'f_05_u': 0.833, 'F1': 0.667, 'overall': 0.69}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.964, 'c@1': 0.818, 'f_05_u': 0.714, 'F1': 0.8, 'overall': 0.824}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.618, 'c@1': 0.549, 'f_05_u': 0.572, 'F1': 0.609, 'overall': 0.587}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.579, 'c@1': 0.574, 'f_05_u': 0.541, 'F1': 0.538, 'overall': 0.558}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.545, 'f_05_u': 0.714, 'F1': 0.615, 'overall': 0.656}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.571, 'c@1': 0.545, 'f_05_u': 0.417, 'F1': 0.444, 'overall': 0.494}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.573, 'c@1': 0.567, 'f_05_u': 0.57, 'F1': 0.529, 'overall': 0.56}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.763, 'c@1': 0.683, 'f_05_u': 0.658, 'F1': 0.667, 'overall': 0.693}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.636, 'f_05_u': 0.833, 'F1': 0.667, 'overall': 0.701}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.857, 'c@1': 0.636, 'f_05_u': 0.536, 'F1': 0.6, 'overall': 0.657}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.752, 'c@1': 0.682, 'f_05_u': 0.673, 'F1': 0.673, 'overall': 0.695}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.812, 'c@1': 0.733, 'f_05_u': 0.721, 'F1': 0.697, 'overall': 0.741}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.818, 'f_05_u': 0.938, 'F1': 0.857, 'overall': 0.872}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.909, 'f_05_u': 0.833, 'F1': 0.889, 'overall': 0.908}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.818, 'c@1': 0.744, 'f_05_u': 0.742, 'F1': 0.708, 'overall': 0.753}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.742, 'c@1': 0.693, 'f_05_u': 0.692, 'F1': 0.587, 'overall': 0.678}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.708, 'c@1': 0.545, 'f_05_u': 0.75, 'F1': 0.545, 'overall': 0.637}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.964, 'c@1': 0.909, 'f_05_u': 0.938, 'F1': 0.857, 'overall': 0.917}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.777, 'c@1': 0.709, 'f_05_u': 0.727, 'F1': 0.61, 'overall': 0.705}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.756, 'c@1': 0.663, 'f_05_u': 0.635, 'F1': 0.702, 'overall': 0.689}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.545, 'f_05_u': 0.694, 'F1': 0.667, 'overall': 0.664}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.571, 'c@1': 0.545, 'f_05_u': 0.417, 'F1': 0.444, 'overall': 0.494}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.705, 'c@1': 0.629, 'f_05_u': 0.627, 'F1': 0.677, 'overall': 0.659}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.663, 'f_05_u': 0.635, 'F1': 0.691, 'overall': 0.685}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.727, 'f_05_u': 0.893, 'F1': 0.769, 'overall': 0.785}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.607, 'c@1': 0.636, 'f_05_u': 0.536, 'F1': 0.6, 'overall': 0.595}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.607, 'c@1': 0.636, 'f_05_u': 0.536, 'F1': 0.6, 'overall': 0.595}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.709, 'c@1': 0.574, 'f_05_u': 0.567, 'F1': 0.639, 'overall': 0.622}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.667, 'c@1': 0.636, 'f_05_u': 0.833, 'F1': 0.667, 'overall': 0.701}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.818, 'f_05_u': 0.714, 'F1': 0.8, 'overall': 0.833}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.684, 'c@1': 0.567, 'f_05_u': 0.585, 'F1': 0.636, 'overall': 0.618}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.794, 'c@1': 0.752, 'f_05_u': 0.736, 'F1': 0.731, 'overall': 0.753}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.833, 'c@1': 0.727, 'f_05_u': 0.833, 'F1': 0.8, 'overall': 0.798}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 0.909, 'f_05_u': 0.833, 'F1': 0.889, 'overall': 0.908}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.809, 'c@1': 0.771, 'f_05_u': 0.759, 'F1': 0.752, 'overall': 0.773}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.757, 'c@1': 0.743, 'f_05_u': 0.791, 'F1': 0.629, 'overall': 0.73}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.542, 'c@1': 0.545, 'f_05_u': 0.75, 'F1': 0.545, 'overall': 0.596}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 1.0, 'c@1': 1.0, 'f_05_u': 1.0, 'F1': 1.0, 'overall': 1.0}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.801, 'c@1': 0.762, 'f_05_u': 0.806, 'F1': 0.667, 'overall': 0.759}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.756, 'c@1': 0.614, 'f_05_u': 0.597, 'F1': 0.672, 'overall': 0.66}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.727, 'f_05_u': 0.833, 'F1': 0.8, 'overall': 0.788}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.727, 'f_05_u': 0.625, 'F1': 0.571, 'overall': 0.668}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.714, 'c@1': 0.602, 'f_05_u': 0.609, 'F1': 0.662, 'overall': 0.647}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.741, 'c@1': 0.604, 'f_05_u': 0.588, 'F1': 0.655, 'overall': 0.647}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.727, 'f_05_u': 0.893, 'F1': 0.769, 'overall': 0.795}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.714, 'c@1': 0.636, 'f_05_u': 0.556, 'F1': 0.667, 'overall': 0.643}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.701, 'c@1': 0.602, 'f_05_u': 0.608, 'F1': 0.651, 'overall': 0.64}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.629, 'c@1': 0.485, 'f_05_u': 0.525, 'F1': 0.629, 'overall': 0.567}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.583, 'c@1': 0.636, 'f_05_u': 0.781, 'F1': 0.714, 'overall': 0.679}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 11 problems in ground truth\n",
      "-> 11 solutions explicitly proposed\n",
      "{'auc': 0.786, 'c@1': 0.455, 'f_05_u': 0.455, 'F1': 0.571, 'overall': 0.567}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 123 problems in ground truth\n",
      "-> 112 solutions explicitly proposed\n",
      "{'auc': 0.576, 'c@1': 0.478, 'f_05_u': 0.544, 'F1': 0.623, 'overall': 0.555}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 468.7611381188035\n",
      "Test for 100 training datapoints Ends:  2020-09-05 22:37:28.199340\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 100 datapoints without discretization - without any range\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 1000 training datapoints Starts:  2020-09-05 20:54:24.897938\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.654, 'c@1': 0.489, 'f_05_u': 0.607, 'F1': 0.722, 'overall': 0.618}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.696, 'c@1': 0.575, 'f_05_u': 0.631, 'F1': 0.786, 'overall': 0.672}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.744, 'c@1': 0.652, 'f_05_u': 0.603, 'F1': 0.796, 'overall': 0.699}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.648, 'c@1': 0.464, 'f_05_u': 0.614, 'F1': 0.723, 'overall': 0.612}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.666, 'c@1': 0.528, 'f_05_u': 0.579, 'F1': 0.758, 'overall': 0.633}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.717, 'c@1': 0.631, 'f_05_u': 0.688, 'F1': 0.737, 'overall': 0.693}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.674, 'c@1': 0.656, 'f_05_u': 0.644, 'F1': 0.721, 'overall': 0.674}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.646, 'c@1': 0.506, 'f_05_u': 0.584, 'F1': 0.761, 'overall': 0.624}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.794, 'c@1': 0.572, 'f_05_u': 0.573, 'F1': 0.74, 'overall': 0.67}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.823, 'c@1': 0.76, 'f_05_u': 0.729, 'F1': 0.901, 'overall': 0.803}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.768, 'c@1': 0.552, 'f_05_u': 0.559, 'F1': 0.708, 'overall': 0.647}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.744, 'c@1': 0.553, 'f_05_u': 0.578, 'F1': 0.738, 'overall': 0.653}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.815, 'c@1': 0.592, 'f_05_u': 0.781, 'F1': 0.074, 'overall': 0.565}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.844, 'c@1': 0.657, 'f_05_u': 0.82, 'F1': 0.64, 'overall': 0.74}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.782, 'c@1': 0.628, 'f_05_u': 0.585, 'F1': 0.868, 'overall': 0.716}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.775, 'c@1': 0.561, 'f_05_u': 0.747, 'F1': 0.235, 'overall': 0.58}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.77, 'c@1': 0.563, 'f_05_u': 0.626, 'F1': 0.142, 'overall': 0.525}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.84, 'c@1': 0.6, 'f_05_u': 0.8, 'F1': 0.44, 'overall': 0.67}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.78, 'c@1': 0.659, 'f_05_u': 0.759, 'F1': 0.381, 'overall': 0.645}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.749, 'c@1': 0.551, 'f_05_u': 0.628, 'F1': 0.148, 'overall': 0.519}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.728, 'c@1': 0.544, 'f_05_u': 0.584, 'F1': 0.831, 'overall': 0.672}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.765, 'c@1': 0.654, 'f_05_u': 0.736, 'F1': 0.831, 'overall': 0.746}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.696, 'c@1': 0.539, 'f_05_u': 0.549, 'F1': 0.708, 'overall': 0.623}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.695, 'c@1': 0.502, 'f_05_u': 0.587, 'F1': 0.826, 'overall': 0.652}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.707, 'c@1': 0.504, 'f_05_u': 0.65, 'F1': 0.82, 'overall': 0.67}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.769, 'c@1': 0.678, 'f_05_u': 0.779, 'F1': 0.706, 'overall': 0.733}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.689, 'c@1': 0.555, 'f_05_u': 0.574, 'F1': 0.747, 'overall': 0.641}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.689, 'c@1': 0.555, 'f_05_u': 0.574, 'F1': 0.747, 'overall': 0.641}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.77, 'c@1': 0.572, 'f_05_u': 0.573, 'F1': 0.757, 'overall': 0.668}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.658, 'f_05_u': 0.72, 'F1': 0.853, 'overall': 0.751}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.763, 'c@1': 0.582, 'f_05_u': 0.57, 'F1': 0.752, 'overall': 0.667}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.728, 'c@1': 0.548, 'f_05_u': 0.578, 'F1': 0.757, 'overall': 0.653}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.827, 'c@1': 0.589, 'f_05_u': 0.775, 'F1': 0.063, 'overall': 0.564}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.847, 'c@1': 0.677, 'f_05_u': 0.845, 'F1': 0.727, 'overall': 0.774}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.776, 'c@1': 0.682, 'f_05_u': 0.674, 'F1': 0.836, 'overall': 0.742}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.8, 'c@1': 0.565, 'f_05_u': 0.748, 'F1': 0.064, 'overall': 0.544}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.784, 'c@1': 0.484, 'f_05_u': 0.028, 'F1': 0.004, 'overall': 0.325}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.839, 'c@1': 0.651, 'f_05_u': 0.841, 'F1': 0.545, 'overall': 0.719}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.806, 'c@1': 0.704, 'f_05_u': 0.825, 'F1': 0.513, 'overall': 0.712}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.754, 'c@1': 0.483, 'f_05_u': 0.302, 'F1': 0.004, 'overall': 0.385}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.723, 'c@1': 0.506, 'f_05_u': 0.587, 'F1': 0.875, 'overall': 0.673}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.758, 'c@1': 0.668, 'f_05_u': 0.703, 'F1': 0.85, 'overall': 0.745}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.782, 'c@1': 0.553, 'f_05_u': 0.549, 'F1': 0.724, 'overall': 0.652}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.701, 'c@1': 0.472, 'f_05_u': 0.591, 'F1': 0.876, 'overall': 0.66}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.699, 'c@1': 0.477, 'f_05_u': 0.62, 'F1': 0.875, 'overall': 0.668}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.81, 'c@1': 0.693, 'f_05_u': 0.794, 'F1': 0.767, 'overall': 0.766}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.739, 'c@1': 0.569, 'f_05_u': 0.549, 'F1': 0.74, 'overall': 0.649}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.681, 'c@1': 0.439, 'f_05_u': 0.621, 'F1': 0.869, 'overall': 0.652}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.63, 'c@1': 0.518, 'f_05_u': 0.572, 'F1': 0.683, 'overall': 0.601}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.644, 'c@1': 0.626, 'f_05_u': 0.655, 'F1': 0.758, 'overall': 0.671}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.694, 'c@1': 0.554, 'f_05_u': 0.558, 'F1': 0.702, 'overall': 0.627}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.602, 'c@1': 0.512, 'f_05_u': 0.577, 'F1': 0.681, 'overall': 0.593}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 4536.363498218358\n",
      "Test for 1000 training datapoints Ends:  2020-09-05 22:10:01.261753\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 1000 datapoints without discretization\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 1000 training datapoints Starts:  2020-09-05 22:49:31.929481\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.698, 'c@1': 0.649, 'f_05_u': 0.662, 'F1': 0.656, 'overall': 0.666}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.734, 'c@1': 0.673, 'f_05_u': 0.724, 'F1': 0.727, 'overall': 0.715}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.744, 'c@1': 0.644, 'f_05_u': 0.637, 'F1': 0.69, 'overall': 0.679}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.693, 'c@1': 0.648, 'f_05_u': 0.661, 'F1': 0.658, 'overall': 0.665}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.675, 'c@1': 0.57, 'f_05_u': 0.599, 'F1': 0.687, 'overall': 0.633}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.716, 'c@1': 0.634, 'f_05_u': 0.706, 'F1': 0.654, 'overall': 0.677}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.687, 'c@1': 0.693, 'f_05_u': 0.682, 'F1': 0.716, 'overall': 0.694}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.65, 'c@1': 0.567, 'f_05_u': 0.603, 'F1': 0.686, 'overall': 0.626}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.795, 'c@1': 0.538, 'f_05_u': 0.583, 'F1': 0.691, 'overall': 0.652}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.839, 'c@1': 0.762, 'f_05_u': 0.791, 'F1': 0.806, 'overall': 0.8}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.564, 'f_05_u': 0.586, 'F1': 0.69, 'overall': 0.653}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.739, 'c@1': 0.533, 'f_05_u': 0.587, 'F1': 0.689, 'overall': 0.637}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.824, 'c@1': 0.569, 'f_05_u': 0.507, 'F1': 0.297, 'overall': 0.549}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.872, 'c@1': 0.713, 'f_05_u': 0.842, 'F1': 0.681, 'overall': 0.777}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.816, 'c@1': 0.663, 'f_05_u': 0.652, 'F1': 0.712, 'overall': 0.711}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.799, 'c@1': 0.61, 'f_05_u': 0.632, 'F1': 0.425, 'overall': 0.617}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.557, 'f_05_u': 0.462, 'F1': 0.258, 'overall': 0.512}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.846, 'c@1': 0.574, 'f_05_u': 0.662, 'F1': 0.456, 'overall': 0.635}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.776, 'c@1': 0.604, 'f_05_u': 0.566, 'F1': 0.375, 'overall': 0.58}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.554, 'f_05_u': 0.521, 'F1': 0.254, 'overall': 0.52}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.758, 'c@1': 0.636, 'f_05_u': 0.641, 'F1': 0.724, 'overall': 0.69}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.77, 'c@1': 0.663, 'f_05_u': 0.719, 'F1': 0.712, 'overall': 0.716}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.693, 'c@1': 0.495, 'f_05_u': 0.549, 'F1': 0.658, 'overall': 0.599}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.723, 'c@1': 0.628, 'f_05_u': 0.639, 'F1': 0.721, 'overall': 0.678}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.768, 'c@1': 0.694, 'f_05_u': 0.706, 'F1': 0.701, 'overall': 0.717}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.769, 'c@1': 0.663, 'f_05_u': 0.76, 'F1': 0.646, 'overall': 0.709}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.706, 'c@1': 0.574, 'f_05_u': 0.587, 'F1': 0.667, 'overall': 0.634}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.706, 'c@1': 0.574, 'f_05_u': 0.587, 'F1': 0.667, 'overall': 0.634}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.773, 'c@1': 0.545, 'f_05_u': 0.587, 'F1': 0.694, 'overall': 0.65}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.815, 'c@1': 0.752, 'f_05_u': 0.804, 'F1': 0.783, 'overall': 0.788}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.776, 'c@1': 0.574, 'f_05_u': 0.592, 'F1': 0.695, 'overall': 0.659}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.723, 'c@1': 0.541, 'f_05_u': 0.591, 'F1': 0.692, 'overall': 0.637}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.838, 'c@1': 0.542, 'f_05_u': 0.396, 'F1': 0.208, 'overall': 0.496}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.733, 'f_05_u': 0.859, 'F1': 0.71, 'overall': 0.794}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.814, 'c@1': 0.743, 'f_05_u': 0.74, 'F1': 0.74, 'overall': 0.759}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.802, 'c@1': 0.539, 'f_05_u': 0.481, 'F1': 0.203, 'overall': 0.506}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.784, 'c@1': 0.484, 'f_05_u': 0.01, 'F1': 0.004, 'overall': 0.32}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.841, 'c@1': 0.624, 'f_05_u': 0.743, 'F1': 0.537, 'overall': 0.686}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.813, 'c@1': 0.703, 'f_05_u': 0.761, 'F1': 0.583, 'overall': 0.715}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.753, 'c@1': 0.482, 'f_05_u': 0.294, 'F1': 0.004, 'overall': 0.383}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.791, 'c@1': 0.667, 'f_05_u': 0.664, 'F1': 0.739, 'overall': 0.715}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.788, 'c@1': 0.713, 'f_05_u': 0.76, 'F1': 0.756, 'overall': 0.754}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.781, 'c@1': 0.535, 'f_05_u': 0.57, 'F1': 0.676, 'overall': 0.64}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.761, 'c@1': 0.66, 'f_05_u': 0.659, 'F1': 0.736, 'overall': 0.704}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.79, 'c@1': 0.716, 'f_05_u': 0.715, 'F1': 0.745, 'overall': 0.742}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.824, 'c@1': 0.723, 'f_05_u': 0.818, 'F1': 0.72, 'overall': 0.771}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.739, 'c@1': 0.525, 'f_05_u': 0.565, 'F1': 0.671, 'overall': 0.625}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.77, 'c@1': 0.707, 'f_05_u': 0.703, 'F1': 0.738, 'overall': 0.73}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.63, 'c@1': 0.516, 'f_05_u': 0.572, 'F1': 0.681, 'overall': 0.6}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.641, 'c@1': 0.604, 'f_05_u': 0.656, 'F1': 0.726, 'overall': 0.657}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.693, 'c@1': 0.515, 'f_05_u': 0.557, 'F1': 0.657, 'overall': 0.605}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.602, 'c@1': 0.511, 'f_05_u': 0.577, 'F1': 0.679, 'overall': 0.592}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 2250.820053048432\n",
      "Test for 1000 training datapoints Ends:  2020-09-05 23:27:02.749787\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 1000 datapoints without discretization -without any range\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 1000 training datapoints Starts:  2020-09-06 02:24:39.094639\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.698, 'c@1': 0.649, 'f_05_u': 0.662, 'F1': 0.656, 'overall': 0.666}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.734, 'c@1': 0.673, 'f_05_u': 0.724, 'F1': 0.727, 'overall': 0.715}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.744, 'c@1': 0.644, 'f_05_u': 0.637, 'F1': 0.69, 'overall': 0.679}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.693, 'c@1': 0.648, 'f_05_u': 0.661, 'F1': 0.658, 'overall': 0.665}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagNounValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.716, 'c@1': 0.491, 'f_05_u': 0.073, 'F1': 0.03, 'overall': 0.327}\n",
      "Evaluation for PosTagNounValue  for Training Data Ends\n",
      "Evaluation for PosTagNounValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.761, 'c@1': 0.693, 'f_05_u': 0.776, 'F1': 0.699, 'overall': 0.732}\n",
      "Evaluation for PosTagNounValue  for Validation Data Ends\n",
      "Evaluation for PosTagNounValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.678, 'c@1': 0.624, 'f_05_u': 0.62, 'F1': 0.627, 'overall': 0.637}\n",
      "Evaluation for PosTagNounValue  for Test Data Ends\n",
      "Evaluation for PosTagNounValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.691, 'c@1': 0.488, 'f_05_u': 0.32, 'F1': 0.028, 'overall': 0.382}\n",
      "Evaluation for PosTagNounValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.675, 'c@1': 0.57, 'f_05_u': 0.599, 'F1': 0.687, 'overall': 0.633}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.716, 'c@1': 0.634, 'f_05_u': 0.706, 'F1': 0.654, 'overall': 0.677}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.687, 'c@1': 0.693, 'f_05_u': 0.682, 'F1': 0.716, 'overall': 0.694}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.65, 'c@1': 0.567, 'f_05_u': 0.603, 'F1': 0.686, 'overall': 0.626}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.634, 'c@1': 0.532, 'f_05_u': 0.386, 'F1': 0.215, 'overall': 0.442}\n",
      "Evaluation for PosTagAdjectiveValue  for Training Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.662, 'c@1': 0.515, 'f_05_u': 0.551, 'F1': 0.38, 'overall': 0.527}\n",
      "Evaluation for PosTagAdjectiveValue  for Validation Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.701, 'c@1': 0.604, 'f_05_u': 0.582, 'F1': 0.459, 'overall': 0.587}\n",
      "Evaluation for PosTagAdjectiveValue  for Test Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.634, 'c@1': 0.53, 'f_05_u': 0.473, 'F1': 0.216, 'overall': 0.463}\n",
      "Evaluation for PosTagAdjectiveValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.795, 'c@1': 0.538, 'f_05_u': 0.583, 'F1': 0.691, 'overall': 0.652}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.839, 'c@1': 0.762, 'f_05_u': 0.791, 'F1': 0.806, 'overall': 0.8}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.564, 'f_05_u': 0.586, 'F1': 0.69, 'overall': 0.653}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.739, 'c@1': 0.533, 'f_05_u': 0.587, 'F1': 0.689, 'overall': 0.637}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.824, 'c@1': 0.569, 'f_05_u': 0.507, 'F1': 0.297, 'overall': 0.549}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.872, 'c@1': 0.713, 'f_05_u': 0.842, 'F1': 0.681, 'overall': 0.777}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.816, 'c@1': 0.663, 'f_05_u': 0.652, 'F1': 0.712, 'overall': 0.711}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.799, 'c@1': 0.61, 'f_05_u': 0.632, 'F1': 0.425, 'overall': 0.617}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.771, 'c@1': 0.557, 'f_05_u': 0.462, 'F1': 0.258, 'overall': 0.512}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.846, 'c@1': 0.574, 'f_05_u': 0.662, 'F1': 0.456, 'overall': 0.635}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.776, 'c@1': 0.604, 'f_05_u': 0.566, 'F1': 0.375, 'overall': 0.58}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.75, 'c@1': 0.554, 'f_05_u': 0.521, 'F1': 0.254, 'overall': 0.52}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.758, 'c@1': 0.636, 'f_05_u': 0.641, 'F1': 0.724, 'overall': 0.69}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.77, 'c@1': 0.663, 'f_05_u': 0.719, 'F1': 0.712, 'overall': 0.716}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.693, 'c@1': 0.495, 'f_05_u': 0.549, 'F1': 0.658, 'overall': 0.599}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.723, 'c@1': 0.628, 'f_05_u': 0.639, 'F1': 0.721, 'overall': 0.678}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.768, 'c@1': 0.694, 'f_05_u': 0.706, 'F1': 0.701, 'overall': 0.717}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.769, 'c@1': 0.663, 'f_05_u': 0.76, 'F1': 0.646, 'overall': 0.709}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.706, 'c@1': 0.574, 'f_05_u': 0.587, 'F1': 0.667, 'overall': 0.634}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.706, 'c@1': 0.574, 'f_05_u': 0.587, 'F1': 0.667, 'overall': 0.634}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Character6Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.76, 'c@1': 0.649, 'f_05_u': 0.685, 'F1': 0.548, 'overall': 0.661}\n",
      "Evaluation for Character6Gram100Value  for Training Data Ends\n",
      "Evaluation for Character6Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.79, 'c@1': 0.673, 'f_05_u': 0.754, 'F1': 0.68, 'overall': 0.724}\n",
      "Evaluation for Character6Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character6Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.672, 'c@1': 0.624, 'f_05_u': 0.62, 'F1': 0.635, 'overall': 0.638}\n",
      "Evaluation for Character6Gram100Value  for Test Data Ends\n",
      "Evaluation for Character6Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.741, 'c@1': 0.639, 'f_05_u': 0.668, 'F1': 0.538, 'overall': 0.646}\n",
      "Evaluation for Character6Gram100Value  for All Data Ends\n",
      "Evaluation for Character7Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.755, 'c@1': 0.564, 'f_05_u': 0.492, 'F1': 0.288, 'overall': 0.525}\n",
      "Evaluation for Character7Gram100Value  for Training Data Ends\n",
      "Evaluation for Character7Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.802, 'c@1': 0.614, 'f_05_u': 0.719, 'F1': 0.541, 'overall': 0.669}\n",
      "Evaluation for Character7Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character7Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.709, 'c@1': 0.644, 'f_05_u': 0.642, 'F1': 0.617, 'overall': 0.653}\n",
      "Evaluation for Character7Gram100Value  for Test Data Ends\n",
      "Evaluation for Character7Gram100Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.709, 'c@1': 0.644, 'f_05_u': 0.642, 'F1': 0.617, 'overall': 0.653}\n",
      "Evaluation for Character7Gram100Value  for All Data Ends\n",
      "Evaluation for Character8Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.752, 'c@1': 0.505, 'f_05_u': 0.189, 'F1': 0.085, 'overall': 0.383}\n",
      "Evaluation for Character8Gram100Value  for Training Data Ends\n",
      "Evaluation for Character8Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.864, 'c@1': 0.683, 'f_05_u': 0.814, 'F1': 0.636, 'overall': 0.749}\n",
      "Evaluation for Character8Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character8Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.758, 'c@1': 0.683, 'f_05_u': 0.702, 'F1': 0.61, 'overall': 0.688}\n",
      "Evaluation for Character8Gram100Value  for Test Data Ends\n",
      "Evaluation for Character8Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.726, 'c@1': 0.504, 'f_05_u': 0.375, 'F1': 0.084, 'overall': 0.422}\n",
      "Evaluation for Character8Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.773, 'c@1': 0.545, 'f_05_u': 0.587, 'F1': 0.694, 'overall': 0.65}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.815, 'c@1': 0.752, 'f_05_u': 0.804, 'F1': 0.783, 'overall': 0.788}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.776, 'c@1': 0.574, 'f_05_u': 0.592, 'F1': 0.695, 'overall': 0.659}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.723, 'c@1': 0.541, 'f_05_u': 0.591, 'F1': 0.692, 'overall': 0.637}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.838, 'c@1': 0.542, 'f_05_u': 0.396, 'F1': 0.208, 'overall': 0.496}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.875, 'c@1': 0.733, 'f_05_u': 0.859, 'F1': 0.71, 'overall': 0.794}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.814, 'c@1': 0.743, 'f_05_u': 0.74, 'F1': 0.74, 'overall': 0.759}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.802, 'c@1': 0.539, 'f_05_u': 0.481, 'F1': 0.203, 'overall': 0.506}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.784, 'c@1': 0.484, 'f_05_u': 0.01, 'F1': 0.004, 'overall': 0.32}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.841, 'c@1': 0.624, 'f_05_u': 0.743, 'F1': 0.537, 'overall': 0.686}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.813, 'c@1': 0.703, 'f_05_u': 0.761, 'F1': 0.583, 'overall': 0.715}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.753, 'c@1': 0.482, 'f_05_u': 0.294, 'F1': 0.004, 'overall': 0.383}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.791, 'c@1': 0.667, 'f_05_u': 0.664, 'F1': 0.739, 'overall': 0.715}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.788, 'c@1': 0.713, 'f_05_u': 0.76, 'F1': 0.756, 'overall': 0.754}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.781, 'c@1': 0.535, 'f_05_u': 0.57, 'F1': 0.676, 'overall': 0.64}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.761, 'c@1': 0.66, 'f_05_u': 0.659, 'F1': 0.736, 'overall': 0.704}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.79, 'c@1': 0.716, 'f_05_u': 0.715, 'F1': 0.745, 'overall': 0.742}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.824, 'c@1': 0.723, 'f_05_u': 0.818, 'F1': 0.72, 'overall': 0.771}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.739, 'c@1': 0.525, 'f_05_u': 0.565, 'F1': 0.671, 'overall': 0.625}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.77, 'c@1': 0.707, 'f_05_u': 0.703, 'F1': 0.738, 'overall': 0.73}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for Character6Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.787, 'c@1': 0.692, 'f_05_u': 0.733, 'F1': 0.64, 'overall': 0.713}\n",
      "Evaluation for Character6Gram200Value  for Training Data Ends\n",
      "Evaluation for Character6Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.827, 'c@1': 0.733, 'f_05_u': 0.819, 'F1': 0.738, 'overall': 0.779}\n",
      "Evaluation for Character6Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character6Gram200Value  for Test Data Starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.728, 'c@1': 0.584, 'f_05_u': 0.596, 'F1': 0.691, 'overall': 0.65}\n",
      "Evaluation for Character6Gram200Value  for Test Data Ends\n",
      "Evaluation for Character6Gram200Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.728, 'c@1': 0.584, 'f_05_u': 0.596, 'F1': 0.691, 'overall': 0.65}\n",
      "Evaluation for Character6Gram200Value  for All Data Ends\n",
      "Evaluation for Character7Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.775, 'c@1': 0.556, 'f_05_u': 0.463, 'F1': 0.262, 'overall': 0.514}\n",
      "Evaluation for Character7Gram200Value  for Training Data Ends\n",
      "Evaluation for Character7Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.848, 'c@1': 0.713, 'f_05_u': 0.833, 'F1': 0.688, 'overall': 0.771}\n",
      "Evaluation for Character7Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character7Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.766, 'c@1': 0.703, 'f_05_u': 0.705, 'F1': 0.688, 'overall': 0.715}\n",
      "Evaluation for Character7Gram200Value  for Test Data Ends\n",
      "Evaluation for Character7Gram200Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.766, 'c@1': 0.703, 'f_05_u': 0.705, 'F1': 0.688, 'overall': 0.715}\n",
      "Evaluation for Character7Gram200Value  for All Data Ends\n",
      "Evaluation for Character8Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.759, 'c@1': 0.495, 'f_05_u': 0.106, 'F1': 0.045, 'overall': 0.351}\n",
      "Evaluation for Character8Gram200Value  for Training Data Ends\n",
      "Evaluation for Character8Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.856, 'c@1': 0.663, 'f_05_u': 0.793, 'F1': 0.605, 'overall': 0.729}\n",
      "Evaluation for Character8Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character8Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.785, 'c@1': 0.733, 'f_05_u': 0.739, 'F1': 0.716, 'overall': 0.743}\n",
      "Evaluation for Character8Gram200Value  for Test Data Ends\n",
      "Evaluation for Character8Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.733, 'c@1': 0.492, 'f_05_u': 0.333, 'F1': 0.041, 'overall': 0.4}\n",
      "Evaluation for Character8Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.63, 'c@1': 0.516, 'f_05_u': 0.572, 'F1': 0.681, 'overall': 0.6}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.641, 'c@1': 0.604, 'f_05_u': 0.656, 'F1': 0.726, 'overall': 0.657}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.693, 'c@1': 0.515, 'f_05_u': 0.557, 'F1': 0.657, 'overall': 0.605}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.602, 'c@1': 0.511, 'f_05_u': 0.577, 'F1': 0.679, 'overall': 0.592}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 2732.2260336354375\n",
      "Test for 1000 training datapoints Ends:  2020-09-06 03:10:11.321607\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 1000 datapoints without discretization -without any range - For all features\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for 1000 training datapoints Starts:  2020-09-06 09:08:16.949797\n",
      "Evaluation for PosTagVerbValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.649, 'c@1': 0.489, 'f_05_u': 0.607, 'F1': 0.722, 'overall': 0.617}\n",
      "Evaluation for PosTagVerbValue  for Training Data Ends\n",
      "Evaluation for PosTagVerbValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.658, 'c@1': 0.575, 'f_05_u': 0.631, 'F1': 0.786, 'overall': 0.662}\n",
      "Evaluation for PosTagVerbValue  for Validation Data Ends\n",
      "Evaluation for PosTagVerbValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.726, 'c@1': 0.652, 'f_05_u': 0.603, 'F1': 0.796, 'overall': 0.694}\n",
      "Evaluation for PosTagVerbValue  for Test Data Ends\n",
      "Evaluation for PosTagVerbValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.644, 'c@1': 0.464, 'f_05_u': 0.614, 'F1': 0.723, 'overall': 0.611}\n",
      "Evaluation for PosTagVerbValue  for All Data Ends\n",
      "Evaluation for PosTagNounValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.581, 'c@1': 0.522, 'f_05_u': 0.51, 'F1': 0.005, 'overall': 0.405}\n",
      "Evaluation for PosTagNounValue  for Training Data Ends\n",
      "Evaluation for PosTagNounValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.721, 'c@1': 0.658, 'f_05_u': 0.76, 'F1': 0.704, 'overall': 0.711}\n",
      "Evaluation for PosTagNounValue  for Validation Data Ends\n",
      "Evaluation for PosTagNounValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.658, 'c@1': 0.604, 'f_05_u': 0.597, 'F1': 0.686, 'overall': 0.636}\n",
      "Evaluation for PosTagNounValue  for Test Data Ends\n",
      "Evaluation for PosTagNounValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.586, 'c@1': 0.513, 'f_05_u': 0.553, 'F1': 0.004, 'overall': 0.414}\n",
      "Evaluation for PosTagNounValue  for All Data Ends\n",
      "Evaluation for PosTagPronounValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.62, 'c@1': 0.528, 'f_05_u': 0.579, 'F1': 0.758, 'overall': 0.621}\n",
      "Evaluation for PosTagPronounValue  for Training Data Ends\n",
      "Evaluation for PosTagPronounValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.694, 'c@1': 0.631, 'f_05_u': 0.688, 'F1': 0.737, 'overall': 0.687}\n",
      "Evaluation for PosTagPronounValue  for Validation Data Ends\n",
      "Evaluation for PosTagPronounValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.682, 'c@1': 0.656, 'f_05_u': 0.644, 'F1': 0.721, 'overall': 0.676}\n",
      "Evaluation for PosTagPronounValue  for Test Data Ends\n",
      "Evaluation for PosTagPronounValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.611, 'c@1': 0.506, 'f_05_u': 0.584, 'F1': 0.761, 'overall': 0.615}\n",
      "Evaluation for PosTagPronounValue  for All Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.59, 'c@1': 0.52, 'f_05_u': 0.571, 'F1': 0.095, 'overall': 0.444}\n",
      "Evaluation for PosTagAdjectiveValue  for Training Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.6, 'c@1': 0.433, 'f_05_u': 0.668, 'F1': 0.158, 'overall': 0.465}\n",
      "Evaluation for PosTagAdjectiveValue  for Validation Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.657, 'c@1': 0.582, 'f_05_u': 0.651, 'F1': 0.439, 'overall': 0.582}\n",
      "Evaluation for PosTagAdjectiveValue  for Test Data Ends\n",
      "Evaluation for PosTagAdjectiveValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.59, 'c@1': 0.503, 'f_05_u': 0.584, 'F1': 0.095, 'overall': 0.443}\n",
      "Evaluation for PosTagAdjectiveValue  for All Data Ends\n",
      "Evaluation for Word1Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.617, 'c@1': 0.572, 'f_05_u': 0.573, 'F1': 0.74, 'overall': 0.625}\n",
      "Evaluation for Word1Gram100Value  for Training Data Ends\n",
      "Evaluation for Word1Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.813, 'c@1': 0.76, 'f_05_u': 0.729, 'F1': 0.901, 'overall': 0.801}\n",
      "Evaluation for Word1Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.597, 'c@1': 0.552, 'f_05_u': 0.559, 'F1': 0.708, 'overall': 0.604}\n",
      "Evaluation for Word1Gram100Value  for Test Data Ends\n",
      "Evaluation for Word1Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.596, 'c@1': 0.553, 'f_05_u': 0.578, 'F1': 0.738, 'overall': 0.616}\n",
      "Evaluation for Word1Gram100Value  for All Data Ends\n",
      "Evaluation for Word2Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.735, 'c@1': 0.592, 'f_05_u': 0.781, 'F1': 0.074, 'overall': 0.545}\n",
      "Evaluation for Word2Gram100Value  for Training Data Ends\n",
      "Evaluation for Word2Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.787, 'c@1': 0.657, 'f_05_u': 0.82, 'F1': 0.64, 'overall': 0.726}\n",
      "Evaluation for Word2Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.769, 'c@1': 0.628, 'f_05_u': 0.585, 'F1': 0.868, 'overall': 0.712}\n",
      "Evaluation for Word2Gram100Value  for Test Data Ends\n",
      "Evaluation for Word2Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.736, 'c@1': 0.561, 'f_05_u': 0.747, 'F1': 0.235, 'overall': 0.57}\n",
      "Evaluation for Word2Gram100Value  for All Data Ends\n",
      "Evaluation for Word3Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.625, 'c@1': 0.563, 'f_05_u': 0.626, 'F1': 0.142, 'overall': 0.489}\n",
      "Evaluation for Word3Gram100Value  for Training Data Ends\n",
      "Evaluation for Word3Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.737, 'c@1': 0.6, 'f_05_u': 0.8, 'F1': 0.44, 'overall': 0.644}\n",
      "Evaluation for Word3Gram100Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.715, 'c@1': 0.659, 'f_05_u': 0.759, 'F1': 0.381, 'overall': 0.629}\n",
      "Evaluation for Word3Gram100Value  for Test Data Ends\n",
      "Evaluation for Word3Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.625, 'c@1': 0.551, 'f_05_u': 0.628, 'F1': 0.148, 'overall': 0.488}\n",
      "Evaluation for Word3Gram100Value  for All Data Ends\n",
      "Evaluation for Character4Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.694, 'c@1': 0.544, 'f_05_u': 0.584, 'F1': 0.831, 'overall': 0.663}\n",
      "Evaluation for Character4Gram100Value  for Training Data Ends\n",
      "Evaluation for Character4Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.751, 'c@1': 0.654, 'f_05_u': 0.736, 'F1': 0.831, 'overall': 0.743}\n",
      "Evaluation for Character4Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.595, 'c@1': 0.539, 'f_05_u': 0.549, 'F1': 0.708, 'overall': 0.598}\n",
      "Evaluation for Character4Gram100Value  for Test Data Ends\n",
      "Evaluation for Character4Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.668, 'c@1': 0.502, 'f_05_u': 0.587, 'F1': 0.826, 'overall': 0.646}\n",
      "Evaluation for Character4Gram100Value  for All Data Ends\n",
      "Evaluation for Character5Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.704, 'c@1': 0.504, 'f_05_u': 0.65, 'F1': 0.82, 'overall': 0.669}\n",
      "Evaluation for Character5Gram100Value  for Training Data Ends\n",
      "Evaluation for Character5Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.752, 'c@1': 0.678, 'f_05_u': 0.779, 'F1': 0.706, 'overall': 0.729}\n",
      "Evaluation for Character5Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.644, 'c@1': 0.555, 'f_05_u': 0.574, 'F1': 0.747, 'overall': 0.63}\n",
      "Evaluation for Character5Gram100Value  for Test Data Ends\n",
      "Evaluation for Character5Gram100Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.644, 'c@1': 0.555, 'f_05_u': 0.574, 'F1': 0.747, 'overall': 0.63}\n",
      "Evaluation for Character5Gram100Value  for All Data Ends\n",
      "Evaluation for Character6Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.709, 'c@1': 0.554, 'f_05_u': 0.701, 'F1': 0.475, 'overall': 0.61}\n",
      "Evaluation for Character6Gram100Value  for Training Data Ends\n",
      "Evaluation for Character6Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.768, 'c@1': 0.711, 'f_05_u': 0.786, 'F1': 0.747, 'overall': 0.753}\n",
      "Evaluation for Character6Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character6Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.645, 'c@1': 0.544, 'f_05_u': 0.594, 'F1': 0.71, 'overall': 0.623}\n",
      "Evaluation for Character6Gram100Value  for Test Data Ends\n",
      "Evaluation for Character6Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.694, 'c@1': 0.522, 'f_05_u': 0.689, 'F1': 0.458, 'overall': 0.591}\n",
      "Evaluation for Character6Gram100Value  for All Data Ends\n",
      "Evaluation for Character7Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.648, 'c@1': 0.566, 'f_05_u': 0.666, 'F1': 0.149, 'overall': 0.507}\n",
      "Evaluation for Character7Gram100Value  for Training Data Ends\n",
      "Evaluation for Character7Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.756, 'c@1': 0.649, 'f_05_u': 0.804, 'F1': 0.586, 'overall': 0.699}\n",
      "Evaluation for Character7Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character7Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.668, 'c@1': 0.585, 'f_05_u': 0.629, 'F1': 0.654, 'overall': 0.634}\n",
      "Evaluation for Character7Gram100Value  for Test Data Ends\n",
      "Evaluation for Character7Gram100Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.668, 'c@1': 0.585, 'f_05_u': 0.629, 'F1': 0.654, 'overall': 0.634}\n",
      "Evaluation for Character7Gram100Value  for All Data Ends\n",
      "Evaluation for Character8Gram100Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.577, 'c@1': 0.523, 'f_05_u': 0.485, 'F1': 0.023, 'overall': 0.402}\n",
      "Evaluation for Character8Gram100Value  for Training Data Ends\n",
      "Evaluation for Character8Gram100Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.782, 'c@1': 0.663, 'f_05_u': 0.849, 'F1': 0.571, 'overall': 0.716}\n",
      "Evaluation for Character8Gram100Value  for Validation Data Ends\n",
      "Evaluation for Character8Gram100Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.72, 'c@1': 0.685, 'f_05_u': 0.686, 'F1': 0.63, 'overall': 0.68}\n",
      "Evaluation for Character8Gram100Value  for Test Data Ends\n",
      "Evaluation for Character8Gram100Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.583, 'c@1': 0.516, 'f_05_u': 0.54, 'F1': 0.025, 'overall': 0.416}\n",
      "Evaluation for Character8Gram100Value  for All Data Ends\n",
      "Evaluation for Word1Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.639, 'c@1': 0.572, 'f_05_u': 0.573, 'F1': 0.757, 'overall': 0.635}\n",
      "Evaluation for Word1Gram200Value  for Training Data Ends\n",
      "Evaluation for Word1Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.758, 'c@1': 0.658, 'f_05_u': 0.72, 'F1': 0.853, 'overall': 0.747}\n",
      "Evaluation for Word1Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word1Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.659, 'c@1': 0.582, 'f_05_u': 0.57, 'F1': 0.752, 'overall': 0.641}\n",
      "Evaluation for Word1Gram200Value  for Test Data Ends\n",
      "Evaluation for Word1Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.62, 'c@1': 0.548, 'f_05_u': 0.578, 'F1': 0.757, 'overall': 0.626}\n",
      "Evaluation for Word1Gram200Value  for All Data Ends\n",
      "Evaluation for Word2Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.727, 'c@1': 0.589, 'f_05_u': 0.775, 'F1': 0.063, 'overall': 0.539}\n",
      "Evaluation for Word2Gram200Value  for Training Data Ends\n",
      "Evaluation for Word2Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.828, 'c@1': 0.677, 'f_05_u': 0.845, 'F1': 0.727, 'overall': 0.77}\n",
      "Evaluation for Word2Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word2Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.777, 'c@1': 0.682, 'f_05_u': 0.674, 'F1': 0.836, 'overall': 0.742}\n",
      "Evaluation for Word2Gram200Value  for Test Data Ends\n",
      "Evaluation for Word2Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.715, 'c@1': 0.565, 'f_05_u': 0.748, 'F1': 0.064, 'overall': 0.523}\n",
      "Evaluation for Word2Gram200Value  for All Data Ends\n",
      "Evaluation for Word3Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.503, 'c@1': 0.484, 'f_05_u': 0.028, 'F1': 0.004, 'overall': 0.255}\n",
      "Evaluation for Word3Gram200Value  for Training Data Ends\n",
      "Evaluation for Word3Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.773, 'c@1': 0.651, 'f_05_u': 0.841, 'F1': 0.545, 'overall': 0.703}\n",
      "Evaluation for Word3Gram200Value  for Validation Data Ends\n",
      "Evaluation for Word3Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.782, 'c@1': 0.704, 'f_05_u': 0.825, 'F1': 0.513, 'overall': 0.706}\n",
      "Evaluation for Word3Gram200Value  for Test Data Ends\n",
      "Evaluation for Word3Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.515, 'c@1': 0.483, 'f_05_u': 0.302, 'F1': 0.004, 'overall': 0.326}\n",
      "Evaluation for Word3Gram200Value  for All Data Ends\n",
      "Evaluation for Character4Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.706, 'c@1': 0.506, 'f_05_u': 0.587, 'F1': 0.875, 'overall': 0.669}\n",
      "Evaluation for Character4Gram200Value  for Training Data Ends\n",
      "Evaluation for Character4Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.741, 'c@1': 0.668, 'f_05_u': 0.703, 'F1': 0.85, 'overall': 0.741}\n",
      "Evaluation for Character4Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character4Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.624, 'c@1': 0.553, 'f_05_u': 0.549, 'F1': 0.724, 'overall': 0.613}\n",
      "Evaluation for Character4Gram200Value  for Test Data Ends\n",
      "Evaluation for Character4Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.688, 'c@1': 0.472, 'f_05_u': 0.591, 'F1': 0.876, 'overall': 0.657}\n",
      "Evaluation for Character4Gram200Value  for All Data Ends\n",
      "Evaluation for Character5Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.696, 'c@1': 0.477, 'f_05_u': 0.62, 'F1': 0.875, 'overall': 0.667}\n",
      "Evaluation for Character5Gram200Value  for Training Data Ends\n",
      "Evaluation for Character5Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.792, 'c@1': 0.693, 'f_05_u': 0.794, 'F1': 0.767, 'overall': 0.761}\n",
      "Evaluation for Character5Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character5Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.653, 'c@1': 0.569, 'f_05_u': 0.549, 'F1': 0.74, 'overall': 0.628}\n",
      "Evaluation for Character5Gram200Value  for Test Data Ends\n",
      "Evaluation for Character5Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.678, 'c@1': 0.439, 'f_05_u': 0.621, 'F1': 0.869, 'overall': 0.652}\n",
      "Evaluation for Character5Gram200Value  for All Data Ends\n",
      "Evaluation for Character6Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.726, 'c@1': 0.53, 'f_05_u': 0.679, 'F1': 0.763, 'overall': 0.674}\n",
      "Evaluation for Character6Gram200Value  for Training Data Ends\n",
      "Evaluation for Character6Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.811, 'c@1': 0.731, 'f_05_u': 0.783, 'F1': 0.853, 'overall': 0.794}\n",
      "Evaluation for Character6Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character6Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.651, 'c@1': 0.556, 'f_05_u': 0.553, 'F1': 0.748, 'overall': 0.627}\n",
      "Evaluation for Character6Gram200Value  for Test Data Ends\n",
      "Evaluation for Character6Gram200Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.651, 'c@1': 0.556, 'f_05_u': 0.553, 'F1': 0.748, 'overall': 0.627}\n",
      "Evaluation for Character6Gram200Value  for All Data Ends\n",
      "Evaluation for Character7Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.67, 'c@1': 0.564, 'f_05_u': 0.698, 'F1': 0.095, 'overall': 0.507}\n",
      "Evaluation for Character7Gram200Value  for Training Data Ends\n",
      "Evaluation for Character7Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.807, 'c@1': 0.688, 'f_05_u': 0.847, 'F1': 0.667, 'overall': 0.752}\n",
      "Evaluation for Character7Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character7Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.734, 'c@1': 0.64, 'f_05_u': 0.652, 'F1': 0.769, 'overall': 0.699}\n",
      "Evaluation for Character7Gram200Value  for Test Data Ends\n",
      "Evaluation for Character7Gram200Value  for All Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.734, 'c@1': 0.64, 'f_05_u': 0.652, 'F1': 0.769, 'overall': 0.699}\n",
      "Evaluation for Character7Gram200Value  for All Data Ends\n",
      "Evaluation for Character8Gram200Value  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.564, 'c@1': 0.515, 'f_05_u': 0.426, 'F1': 0.004, 'overall': 0.377}\n",
      "Evaluation for Character8Gram200Value  for Training Data Ends\n",
      "Evaluation for Character8Gram200Value  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.765, 'c@1': 0.651, 'f_05_u': 0.825, 'F1': 0.561, 'overall': 0.701}\n",
      "Evaluation for Character8Gram200Value  for Validation Data Ends\n",
      "Evaluation for Character8Gram200Value  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.763, 'c@1': 0.728, 'f_05_u': 0.679, 'F1': 0.784, 'overall': 0.738}\n",
      "Evaluation for Character8Gram200Value  for Test Data Ends\n",
      "Evaluation for Character8Gram200Value  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.57, 'c@1': 0.508, 'f_05_u': 0.504, 'F1': 0.004, 'overall': 0.397}\n",
      "Evaluation for Character8Gram200Value  for All Data Ends\n",
      "Evaluation for PosTagCountValue  for Training Data Starts\n",
      "-> 1001 problems in ground truth\n",
      "-> 1001 solutions explicitly proposed\n",
      "{'auc': 0.502, 'c@1': 0.518, 'f_05_u': 0.572, 'F1': 0.683, 'overall': 0.569}\n",
      "Evaluation for PosTagCountValue  for Training Data Ends\n",
      "Evaluation for PosTagCountValue  for Validation Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.585, 'c@1': 0.626, 'f_05_u': 0.655, 'F1': 0.758, 'overall': 0.656}\n",
      "Evaluation for PosTagCountValue  for Validation Data Ends\n",
      "Evaluation for PosTagCountValue  for Test Data Starts\n",
      "-> 101 problems in ground truth\n",
      "-> 101 solutions explicitly proposed\n",
      "{'auc': 0.595, 'c@1': 0.554, 'f_05_u': 0.558, 'F1': 0.702, 'overall': 0.602}\n",
      "Evaluation for PosTagCountValue  for Test Data Ends\n",
      "Evaluation for PosTagCountValue  for All Data Starts\n",
      "-> 1203 problems in ground truth\n",
      "-> 1102 solutions explicitly proposed\n",
      "{'auc': 0.49, 'c@1': 0.512, 'f_05_u': 0.577, 'F1': 0.681, 'overall': 0.565}\n",
      "Evaluation for PosTagCountValue  for All Data Ends\n",
      "Time taken: 5032.3564579337835\n",
      "Test for 1000 training datapoints Ends:  2020-09-06 10:32:09.306538\n"
     ]
    }
   ],
   "source": [
    "# Do not run these cells\n",
    "# For 1000 datapoints with discretization - For all features\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
